\chapter{Introduction}
\label{chapter:introduction}

Classical planning provides a method for representing and solving various problems. Formulating these problems as planning tasks can model real-world challenges such as route planning, robotics, automated system verification, and computational biology~\cite{edelkamp2012heuristic}. This approach enables automated systems to reason, make decisions, and generate plans to achieve specific objectives. The ability to represent and solve problems using classical planning techniques has garnered significant attention and has proven instrumental in various domains.

Planning tasks are typically defined by the initial state and the desired outcome (goal state). States capture the relevant information about the system's condition at a particular time. Each domain provides a set of actions that describe how a state can be transformed. A plan is constructed to reach the goal from the initial state by applying a sequence of actions. The plan provides a step-by-step guide for an agent or a system to follow to achieve the desired objective. By using various planning techniques and algorithms, planning systems can efficiently explore the space and generate plans to solve complex problems, including those classified as PSPACE-complete~\cite{bylander1994computational}.

Several approaches can find a sequence of actions that transforms an initial state into one that satisfies the goal condition. One successful strategy for solving such tasks is to apply algorithms of the best-first search family, which use a function~$f$ to guide the search. For a given state~$s$, the function~$f$ can combine the current cost~$g(s)$ of a partial sequence of actions starting in the initial state and resulting in~$s$, and a heuristic function~$h(s)$ that estimates the cost-to-goal from state~$s$. For example, \astar search is guided by $f(s)=g(s)+h(s)$~\cite{hart1968formal}, and greedy best-first search is guided only by $f(s)=h(s)$~\cite{doran1966experiments}. Generally, best-first search algorithms are more effective when the heuristic function~\h better estimates the perfect cost-to-goal.

Logic-based relaxations of planning tasks create some of the most successful heuristic functions, e.g.,~the delete relaxation~\cite{hoffmann2001ff}, critical paths~\cite{haslum2004admissible}, landmarks~\cite{hoffmann2004ordered,karpas2009cost}, or the state equation~\cite{bonet2013admissible}. Many of these heuristics come with additional properties, such as admissibility.

The emergence of interest in learning heuristic functions with neural networks has been driven by rapid progress in other application areas. Some works in this area include those by~\citet{samadi2008learning,arfaee2011learning,agostinelli2019solving,yu2020learning,shen2020learning,ferber2020neural,toyer2020asnets,ferber2022neural}, and~\citet{otoole2022sampling}. The basic approach is simple: one generates a set of samples of pairs of states and estimates of cost-to-goal and trains a supervised model over the set of samples. However, a successful approach to planning has to solve several challenges:

\begin{enumerate}[label=C\arabic*),left=0pt]
    \itemsep0pt
    \item State spaces are implicitly defined and mostly exponential in the size of a compact description. Therefore, random samples are hard to generate and may be infeasible, unreachable from an initial state, or unable to reach the goal. Samples are usually generated by expanding the state space through forward or backward search~(regression).
    \item Estimates of cost-to-goal are typically hard to obtain. Finding the perfect cost-to-goal amounts to solving the task on the samples, and often the learned function is only useful if it is close to the perfect cost-to-goal.
    \item Planning domains are very different, and logic-based heuristics apply to any domain. This results in the problem of transferring a learned heuristic to new domains, tasks, or state spaces.
    \item Planners depend on evaluating many states per second, so computing the heuristic function should be fast, or the learned heuristic must be more informed. However, there is a trade-off between a more informed learned heuristic and the complexity of the model.
\end{enumerate}

By addressing these core issues, new possibilities can be found for advancing the field of classical planning and enabling more efficient and effective problem-solving capabilities in planning systems. For this, we investigate current state sampling methods and propose new techniques that directly affect the quality of samples for training heuristic functions.

\section{Contributions}
\label{sec:contributions}

Through controlled experiments on planning tasks with small state spaces, we identify several techniques that improve the quality of the samples used for training. Our contributions include the following:

\begin{itemize}
    \item A sample generation algorithm that can better represent a relevant subset of the state space through a combination of breadth-first search~(expanding states close to the goal) followed by random walks from the breadth-first search's leaves~(\cref{sec:regression}).
    \item On-the-fly state space-based estimations to limit the sampling regression depth to large big cost-to-goal overestimates~(\cref{sec:rollout-limit}).
    \item Two methods to improve cost-to-goal estimates based on detecting samples from the same or neighboring states~(\cref{sec:cost-to-goal-estimates}).
    \item A systematic study on sampling quality~(\cref{sec:experiments}).
\end{itemize}

\section{Overview and Outline}
\label{sec:outline}

This dissertation explores strategies for generating samples and their influence on heuristic function performance. In \cref{sec:background}, we provide the necessary background for our work, covering important topics such as classical planning~(\cref{sec:classical-planning}), search algorithms~(\cref{sec:search-algorithms}), and neural networks~(\cref{sec:neural-networks}). Additionally, we review relevant previous research and related work in \cref{sec:related-work}. By establishing this, we present the various techniques of sample generation and propose new approaches to improve them in \cref{sec:sampling}. We focus mainly on the quality of the learned heuristic and its influence on the number of expanded states and coverage. To this end, \cref{sec:experiments} presents a systematic study of the contributions of each strategy when solving distinct initial states of a single state space, aiming to understand better how to learn high-quality heuristics. In \cref{sec:settings}, we present our settings to learn state-space-specific heuristics using a feedforward neural network. In experiments on small state spaces in \cref{sec:small-experiments}, we investigate the effect of different sampling strategies, the quality of the learned heuristic with an increasing number of samples, and the effect of a different subset of states part of the sample set on the learned heuristic; we also evaluate how the quality of the estimates of cost-to-goal influences the effectiveness of learned heuristic to guide a search algorithm. Then, in \cref{sec:large-experiments}, we compare our best strategy with a baseline and traditional logic-based methods over large state spaces. Furthermore, we qualitatively compare existing methods in \cref{sec:large-exps-comparison} and discuss the limitations of learned heuristics in \cref{sec:limitations}. Finally, we conclude and highlight possible future works in \cref{sec:conclusion}.
