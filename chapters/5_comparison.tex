\chapter{Comparison to Other Methods}
\label{sec:comparison}

Although we do not aim to make a systematic comparison to other methods due to distinct machine configurations, libraries or missing source code, we try to qualitatively compare our work with Ferber et al.~(\citeyear{Ferber.etal/2022}) and O'Toole et al.~(\citeyear{OToole/2022}). All methods share the same dataset and NN configuration -- differences include batch size, patience value, NN initialization functions, and percentages of data split into training and validation sets.

Regarding our method, the sampling and training procedures can take a combined time of up to $2$ hours, and we use a search time limit of $5$ minutes. Ferber et al.~(\citeyear{Ferber.etal/2022}) perform sampling and training for up to $28$ hours, with a search time-limit of $10$ hours. O'Toole et al.~(\citeyear{OToole/2022}) spend an unreported amount of time to generate $100$\,K samples and an average of $23$ minutes in training, with a search time-limit of $6$ minutes. In the case of Ferber et al.~(\citeyear{Ferber.etal/2022}), they use a validation method that consists in retraining the network up to three times if the learned heuristic is not able to solve with \gbfs more than $80\,\%$ of generated validation states with a search time-limit of $30$ minutes. Meanwhile, O'Toole et al.~(\citeyear{OToole/2022}) train $10$ networks for each state space and select the one with the best performing heuristic according to the same validation method as Ferber et al.~(\citeyear{Ferber.etal/2022}). We do not use any form of validation since we report the mean of multiple seeds.

Considering only the best configurations, Ferber et al.~(\citeyear{Ferber.etal/2022}) and O'Toole et al.~(\citeyear{OToole/2022}) have the sampling procedure based on backward searches from the goal with random walk, while we use a combination of breadth-first search with random walk. Ferber et al.~(\citeyear{Ferber.etal/2022}) obtain states through regression and try to solve them using \gbfs with the current heuristic to obtain the plans, which are used as training data. They use bootstrapping, which consists of improving the learned heuristics by training with samples of increasing difficulty, starting with a random walk limit of $5$ and doubling it (up to $8$ times) whenever \gbfs finds a plan for at least $95\,\%$ of the states obtained through regression. O'Toole et al.~(\citeyear{OToole/2022}) perform $5$ rollouts with a regression limit of $L=500$ and use the current depth as cost-to-goal estimates; they use the Tarski planning framework to perform the regression procedure as opposed to Fast Downward and developed a cost-to-goal improvement strategy equivalent to \hmin, and $50\,\%$ of the samples are randomly generated as described in Section~\ref{sec:random-samples-theory}. All methods have a boolean representation for the samples and use mutexes (obtained from Fast Downward) to complete unassigned state variables.

We now compare the coverage results between all methods over the same tasks. We perform an additional set of experiments to show in Table~\ref{tab:large-instances-moderate-100k} the coverage results of our best method using $0\,\%$, $20\,\%$ and $50\,\%$ of random samples, obtained from training over $100$\,K samples -- same quantity as O'Toole et al.~(\citeyear{OToole/2022}). We also show results extracted from Ferber et al.~(\citeyear{Ferber.etal/2022}) \hboot and O'Toole et al.~(\citeyear{OToole/2022}) \hnrsl, both trained without validation; we did not re-run their experiments. Note that although the dataset is the same, the results are not fully comparable due to different machine configurations and time dedicated to sampling, training, and testing. Considering coverages, we notice that our methods have results more similar to \hnrsl than to \hboot, and except for Grid, Rovers, Scanalyzer and Storage, a higher coverage. Generating samples only through regression (i.e., without solving states) and training afterward is faster when compared to bootstrapping: a significant limitation of it is the high cost of generating samples, as the states generated by the backward random walk must be solved with the currently learned heuristic to produce plans used as samples. As it stands, both \hnrsl and our methods suggest that sampling using regression with improvement strategies (such as \hmin, \hvfc and random samples) gives competitive results on most domains.

\input{tables/large-instances-moderate-100k}

According to O'Toole et al.~(\citeyear{OToole/2022}), the proportion of random samples in the final sample set has the most positive effect on coverage -- approximately doubling it when going from $0\,\%$ to $50\,\%$ of random samples ($34.7$ vs.~$59.9$, from their supplementary material). As seen in Table~\ref{tab:large-instances-moderate-100k}, we also notice an improvement from using random samples, although not as substantial. This improvement is more prominent in O'Toole et al.~(\citeyear{OToole/2022}), most likely due to their small quantity of rollouts (only five), negatively influencing sample diversity, which is compensated by adding random samples. In our experiments, we see that all domains either improve or have similar results, and the mean coverage improves by about $10\,\%$. Also, except for Pipes-NT, we saw no improvements above $5\,\%$ using $50\,\%$ of random samples compared to $20\,\%$. This means that despite having the highest coverage with $20\,\%$, using $50\,\%$ of random samples can maintain similar coverage while improving sampling time, as random samples are generated faster than in regression and are not computed in the $h$-value improvement strategies.
