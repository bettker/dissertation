\chapter{Sample Generation}
\label{sec:sampling}

Our objective is to conduct a systematic investigation of sample generation methods to understand the influence of each modification on the performance of sampling strategies. We explore the effects of each component in sample generation and propose new strategies to improve the performance of the learned heuristic. By systematically analyzing and comparing the outcomes of each strategy, we can gain insights into the strengths and weaknesses of different sample generation approaches. This understanding will allow us to identify how each technique contributes to enhancing performance and guide us to a better learned heuristic function.

Therefore, we focus on how three aspects of sample generation influence the performance of the learned heuristic to guide a search algorithm: the distribution of states~$s_i$ in the state space, the quality of the estimates~$h_i$ of samples with respect to the $\hstar$-value. Learning a heuristic function requires a set of samples~$(s_1,h_1),\ldots,(s_N,h_N)$, where each sample~$(s_i,h_i), i\in[N]$ consists of a state~$s_i$ and a cost-to-goal estimate~$h_i$.

We restrict our study to generalizing over planning tasks with initial states part of the same FSS and a fixed goal condition. We study model-free approaches with access to predecessors and successors of partial states through a black-box function, to the goal condition, and to the domain of each variable. We also study mutex-based approaches with access to mutexes derived from Fast~Downward~\cite{helmert2006fast}. We address the generation of states in \cref{sec:state-generation} and the estimation of the cost-to-goal in \cref{sec:cost-to-goal-estimates}. In both sections, we explore different techniques from the literature and introduce novel methods. The methods are a sampling strategy combining regression by breadth-first search with random walk~(RW), an adaptive regression limit based on the input task parameters, and two improvement methods for cost-to-goal estimates. Finally, \cref{sec:workflow} presents the workflow of all the techniques discussed and an example of their applications in the Blocks~World domain.

\section{Generation of States}
\label{sec:state-generation}

Unlike other domains in machine learning, such as image recognition or natural language processing, where datasets of samples are often collected in real-world experiments and subsequently need to be manually annotated and carefully curated, the task of sample generation in classical planning presents a particular problem. In contrast to the reliance on external data sources, classical planning uses the structure of the task domain. Thus, the generation of samples becomes an algorithmic problem that uses the availability of the state space and the ability to compute cost-to-goal estimates. This allows for a systematic and controlled investigation of various techniques using only computational resources.

The literature's approaches to generating states typically address three fundamental approaches: progression from one or more initial states, random sampling of the state space, or regression from a goal state. Progression and regression involve starting from a state and systematically applying forward~(progression) or backward~(regression) operators to expand the state space. Various expansion strategies, such as random walks, BFS and DFS, or teacher searches, can be used, including searches using reinforcement learning or bootstrapping~\cite{arfaee2011learning}. Each strategy influences the coverage and expansion patterns of the generated states, contributing to the overall diversity and representation of the sample set.

A problem in both random sampling and progression-based methods is obtaining the cost-to-goal estimates. Without access to efficiently computable heuristic functions, or in model-free approaches, obtaining these values is a challenging task. Typically, search algorithms must be used to generate these estimates, which can introduce significant computational costs in tasks with large state spaces.

To remain less dependent on models than logic-based methods and more general, we focus on regression for which an upper bound on the cost-to-goal is readily available, discussed in \cref{sec:regression,sec:rollout-limit}. Regression leads to partial states, so the problem of generating complete states is addressed in \cref{sec:state-completion}. Using random sampling as an additional approach is discussed in \cref{sec:random-samples}.

\subsection{Sampling by Regression}
\label{sec:regression}

To generate samples, we expand states from the backward state space through regression using the main techniques in the literature, such as expansion by BFS, DFS, and RW. Regression sampling consists of one or more regression rollouts involving a series of state expansions. A rollout starts in an arbitrary state and continues until the last expanded state has no predecessors, or a maximum regression limit is reached. Therefore, RW can have multiple rollouts, while \bfs and \dfs only have one. The sampling generation stops when the total number of samples $N$ has been reached.

During expansion, we optionally use mutexes obtained from an analysis of the planning task -- in our case, as computed by Fast~Downward~\cite{helmert2006fast}~-- to avoid partial states which cannot be completed to complete states without violating a mutex. We also avoid repeated partial states for random walk rollouts, such that a single rollout never cycles. However, the same partial state can be sampled multiple times in different rollouts.

Regression starts from the goal~$s^*$ with the cost-to-goal estimate~$h(s^*)=0$. When applying a backward operator~$o_r$ to a state~$s$, the resulting state~$s' = \pred(s, o_r)$ receives a cost-to-goal estimate of $h(s') = h(s) + \text{cost}(o_r)$. For a state~$s$ generated during rollout that satisfies the goal condition, i.e.,~$s \subseteq s^*$, we set its cost-to-goal estimate to~$0$. States are added to the sample set when generated in the random walks and when expanded in BFS or DFS. In all methods, backward operators applicable to a state are applied randomly.

Using different expansion strategies and considering multiple rollouts, we aim to generate a diverse and representative sample set of the state space. Different expansion strategies generate sample sets with different distributions over the state space. In our experience, good coverage of states close to the goal, such as those obtained by BFS or random walks, is valuable, as is the greater depth obtained by DFS or random walks. However, random walks from a goal state often sample states close to the goal multiple times, while DFS can result in samples that are distant. Based on these observations, we propose a novel combination of BFS and random walks called Focused Sampling Method (\bfsrw) that aims to have a good coverage close to the goal, from the BFS, and a diverse set of samples from the remaining state space, from the random walks. By integrating these techniques, \bfsrw aims to achieve a balanced and comprehensive sampling of the state space. \cref{alg:fsm} presents the approach.

\begin{algorithm}[tb]
    \caption{FSM algorithm}
    \label{alg:fsm}

    \SetKwData{N}{N}
    \SetKwData{Nbfs}{N\textsubscript{\bfs}}
    \SetKwData{bfspct}{p\textsubscript{\bfsrw}}
    \SetKwData{initstate}{s\textsubscript{0}}
    \SetKwData{S}{S}
    \SetKwData{Di}{D\textsubscript{k}}
    \SetKwData{Dj}{D\textsubscript{k+1}}
    \SetKwData{s}{s}
    \SetKwData{sp}{s'}
    \SetKwData{t}{t}
    \SetKwData{O}{Q}
    \SetKwData{L}{L}
    \SetKwData{Srw}{S\textsubscript{RW}}

    \KwData{
        \N: number of samples;
        \bfspct: fraction of samples obtained by BFS;
        \initstate: initial state;
        \L: random walk rollout limit
    }
    \KwResult{\S: the sample set}
    \SetKwProg{Fn}{Function}{:}{}

    \SetKwFunction{FSM}{FSM}
    \Fn{\FSM{\bfspct, \initstate, \L}}{
        $\S \gets \{\initstate\}$ \\
        $\O \gets \{\initstate\}$ \\
        $\Di \gets \{\initstate\}$ \tcp*[h]{current layer} \\
        $\Dj \gets \emptyset$ \tcp*[h]{next layer} \\
        $\Nbfs \gets \N \times \bfspct$ \\

        \tcp{BFS phase}
        \While{$\lvert \S \rvert < \Nbfs$ \textnormal{\textbf{and}} $\Di \neq \emptyset$}{
            Shuffles \Di with a uniform distribution \\
            \ForEach{$\s \in \Di$}{
                $\t \gets \{\sp \mid \sp \in \pred(s)\ \textbf{and}\ \sp \notin \S$\} \\
                \If{$\lvert \S \rvert + \lvert \t \rvert \le \Nbfs$}{
                    $\Dj \gets \Dj \cup \t$ \\
                    $\S \gets \S \cup \t$ \\
                    $\O \gets \left(\O \cup \t \right) \backslash\{\s\}$ \\
                }
            }
            $\Di \gets \Dj$ \\
        }

        \tcp{RW phase}
        \While{$\lvert \S \rvert < \N$}{
            Shuffles \O with a uniform distribution \\
            \ForEach{$\s \in \O$}{
                $\Srw \gets$ set of $\min(\L - \hstar(\s), \N - \lvert \S \rvert)$ states sampled via RW starting from $\s$ \\
                $\S \gets \S \cup \Srw$ \\
                \lIf{$\lvert \S \rvert = \N$}{break}
            }
        }

        \Return \S \\
    }
\end{algorithm}

FSM has two phases. In the first phase, a fixed percentage~$p_\bfsrw$ of the desired $N$~samples is generated using BFS. The value of $p_\bfsrw$ also serves as a constraint on computational resources, and the BFS process terminates when either the desired number of samples or the resource limit is reached first. \bfs expands a state from layer~$k$ and generates $n$~states from layer~$k+1$~(line~11). Each generated state is inserted into the sample set~$S$ if the current total samples plus $n$ are within $p_{\bfsrw}N$ samples; otherwise, no states are sampled, and \bfs expands another state. When expanding an entire layer, the algorithm moves to the next layer.

Let $Q$ be the states in $S$ that did not generate successors for the sample set, i.e.,~they were not expanded, or their set of successors did not fit. The second phase generates multiple random walk rollouts, each starting from a state $s \in Q$ chosen randomly with a complete replacement after all states have been selected once. Each rollout samples $L-\hstar(s)$ states, where $L$ is the maximum regression limit and $\hstar(s)$ is the distance already traversed by BFS. During a random walk, states sampled in the BFS phase are avoided. This is repeated until reaching $N$~samples in the sample set.

\subsection{Maximum Regression Limit}
\label{sec:rollout-limit}

The maximum regression limit controls the depth of the rollout during sample generation. This limit determines how far the sampling can deviate from the goal during regression. In random walk, the regression limit is represented by the maximum length (number of random steps) the algorithm can take in a rollout. In BFS or DFS, the maximum regression limit corresponds to the maximum depth (distance from the goal) that can be reached.

This limit serves two main objectives. First, particularly in regression-based methods, it helps maintain the precision of the cost-to-goal estimate, which is determined by the distance from the goal state that the sample was generated and tends to degrade during sampling due to the random nature of algorithms such as random walk or DFS. Second, it regulates the distribution of samples by periodically restarting the sampling, effectively distributing them across the different distances from the goal.

By establishing an appropriate maximum regression limit, we can balance the expansion of the state space and precision in the cost-to-goal estimates. If the limit is too low, the sampling may fail to cover areas far from the goal, typically where the initial states of a search reside, resulting in a sample set lacking information from these regions. On the other hand, if the limit is too large, the sampling may extend too far from the goal, increasing the error on the cost-to-goal estimates.

A simple strategy is to define some maximum limit~$L$. This has been used in previous work, e.g.,~by \citet{yu2020learning} and \citet{otoole2022sampling} with $L=200$ and $L=500$, respectively. A fixed limit is not the optimal choice for tasks with state spaces of different maximum distances to a fixed goal when we aim for a representative sample of the state space. Therefore, the use of adaptive strategies is important to dynamically adjust the maximum rollout limit, ensuring the generation of a representative sample set that captures the characteristics of the state space.

The ideal regression limit is the one that stops regression when reaching the furthest state from the goal. Let \ssdiameter denote the distance from the goal to its farthest state. For BFS, \ssdiameter would be the ideal estimate; for DFS and random walks, higher limits are required since they do not follow the shortest paths. Since \ssdiameter, in general, is unknown, a potential solution is to estimate this value using available or domain-specific information.

We propose two adaptive and approximate methods to define a suitable maximum regression limit based on the input task parameters. The first method uses the number of facts $F = |\mathcal{F}(s_0)|$ to estimate the \ssdiameter. Since each state is represented by a set of facts $\mathcal{F}$, we assume that each operator changes exactly one fact in the worst-case scenario. Therefore, starting from a particular state, it is possible to modify all the facts by applying $F$ operators and thus reaching any other state in the state space.

However, operators can modify more than one fact at a time, allowing us to refine the estimate by considering the average number of facts changed by the task's operators. Therefore, we propose the second method, referred to as~$$\bar F=\ceil{\frac{F}{\sum_{o\in \mathcal{O}} \frac{|\eff(o)|}{|\mathcal{O}|}}},$$ i.e.,~the number of facts per mean number of effects in the operators. This technique generates a more refined estimate of the \ssdiameter.

Using these adaptive and approximated methods, we can define the maximum regression limit based on the characteristics of each planning task. The regression limit~\facts offers a simple yet effective estimation, while the \meanfx provides a more refined approximation.

\subsection{State Completion}
\label{sec:state-completion}

The goal condition is represented as a partial state, consisting of a subset of facts of a complete state that need to be satisfied to reach the desired solution. Therefore, sampling by regression generates a set of partial states. Since NNs use complete states as input during the search, it becomes necessary to perform state completion, which involves assigning values to the undefined variables in the partial states. By using additional information and making more informed decisions during the state completion, we can investigate the gain it can have on the learning of the NN.

We introduce three approaches to complete a partial state. The first one is a random assignment, where each partial state can be completed by assigning a value~$s(v) \in \dom(v)$ to all fact pairs~$(v,s(v))$ where $s(v)=\bot$. This completion strategy is model-free since it relies solely on the available domain values without making additional assumptions.

The second method is mutex-based and aims to avoid states that are impossible to reach during the search. It also assigns a random value~$s(v) \in \dom(v)$ to each undefined variable~$v$ but excludes states that do not satisfy mutexes. This is achieved by rejection sampling, where the undefined variables are processed in random order and set to a random value that does not violate the mutexes. If the state cannot be completed, we leave the facts undefined, i.e.,~set to false. By using the mutex information, we generate states more consistent with those in the FSS, enhancing the quality of the training data.

While this mutex-based solution provides enhanced state completion, generating invalid states or states that cannot be reached during the search is still possible. We introduce an ideal state completion method to investigate the influence of generating only those states that are reachable from the initial states of interest. This method aims to produce a sample set that is highly relevant to the specific task. To complete a partial state~$s$, we sample a random state from $s~\cap$~FSS. Since we sample by regression, for some states, $s~\cap$~FSS may be empty; such states are avoided during regression. However, it is important to note that this method applies only to small tasks where we can enumerate the complete FSS of the initial state~$s_0$.

\subsection{Randomly Generated Samples}
\label{sec:random-samples}

We explore random sampling, as proposed by \citet{otoole2022sampling}, as an additional approach to generate samples. They have shown that adding randomly generated samples to a set of samples generated by expansion improves the performance of the learned heuristic. In their work, they propose to set the cost-to-goal estimate for these samples to $L+1$ for a maximum regression limit of $L$.

We use this method to investigate the influence of randomly generated samples on the sample set. These samples are generated from a fully undefined state $s$, i.e., $s(v) = \bot$ for all $v \in \mathcal{V}$, using the mutex-based state completion described in the previous section. When generating a state~$s$ by random sampling, if $s$ is already present in the sample set, it receives the same cost-to-goal estimate; otherwise, it receives a cost estimate of $1+\max_{i\in[N]} h_i$ that is larger than all sample estimates. By using random samples, we aim to investigate their influence on the overall performance of the sampling approach.

\section{Improving Cost-to-Goal Estimates}
\label{sec:cost-to-goal-estimates}

The cost-to-goal estimate's accuracy also influences the quality of the sample set. To improve the cost-to-goal estimate, the objective is to reduce the difference from the actual goal distance~\hstar. Our sampling approach ensures that the cost-to-goal estimates assigned to the samples are based on the operators used to reach that particular state. Thus, we guarantee that a sample never underestimates \hstar.

\newpage

\begin{property}
    \label{prop:hvalue}
    The cost-to-goal estimate~$h(s)$ of a sample~$s$ obtained by regression satisfies $h(s)\geq h^*(s)$.
\end{property}
\begin{proof}
    This follows because each estimate is witnessed by a plan. As observed in \cref{sec:background}, a valid regression sequence $\rho=(o_1,\ldots,o_k)$ generates a sequence of partial states that can reach the goal in at most $k$~steps and with a cost at most $\sum_{i\in[k]}\text{cost}(o_i)$, which cannot be lower than the optimal cost.
\end{proof}

Therefore, we apply two procedures that improve the cost-to-goal estimates but maintain \cref{prop:hvalue}. The first, dubbed SAI, minimizes estimates over repeated samples, and the second, SUI, over successors of samples. Both are model-free techniques.

\subsection{Improvement of Repeated Samples}
\label{sec:sai}

When generating samples through multiple rollouts, there is a possibility of overlap, where the same state could appear at different distances across different rollouts, leading to varying cost-to-goal estimates. When training the NN, different labels for the same sample can result in inconsistency and affect learning. To address this issue and improve the quality of cost-to-goal estimates, we propose Sample Improvement (SAI).

SAI aims to improve the consistency of the training data by selecting a single cost-to-goal estimate for each unique state. \cref{prop:hvalue} guarantees that no sampled cost-to-goal estimate underestimates the actual goal distance~\hstar, so the lower the value, the closer it is to \hstar. Therefore, we choose the sample with the lowest cost-to-goal estimate to represent each particular state.

For all sampled states~$s$ we update each cost-to-goal estimate to the best estimate~$h(s) = \min\{h_i \mid s=s_i, i\in[N]\}$. Since different partial states can generate identical complete states, the improvement is applied to partial and complete states. Choosing the minimum \h-value is sound since, in all cases, we have valid plans from a regression that witness these distances; for the same reason, \cref{prop:hvalue} still holds.

\begin{figure}[tb]
    \caption[SAI technique applied on samples of random walk rollouts.]{SAI technique applied on samples of two random walk rollouts. Each node represents a state, and each arc is an applicable operator. Each rollout has a color, and its operators are colored accordingly.}
    \label{fig:sai}
    \addmargin
    \centering
    \input{figures/sai}
\end{figure}

\cref{fig:sai} presents the technique applied to sampling from random walks. Both rollouts generate the state~$s_2$ but through different paths. The red rollout reaches it with a distance of $1$ from the goal state~$s^*$, and the blue rollout generates $s_2$ after $3$~hops. Consequently, we have two samples $(s_2,1)$ and $(s_2,3)$. Applying the SAI technique, we update the cost-to-goal estimate~$h(s_2)$ of both samples to $\min(1,3)=1$. The same process is applied to samples of state~$s_8$, which receives $h(s_8)=\min(5,7)=5$.

\subsection{Improvement over Successors}
\label{sec:sui}

In addition to sampling the same states in different rollouts, it is common to sample states neighbors in the state space. This is more frequent in states at the beginning of the rollout. By sampling neighboring states, we can use the local information to enhance the accuracy of the cost-to-goal estimates. For this, we propose a technique called Successor Improvement (SUI).

SUI uses the fact that neighboring samples, distant by an operator, can be connected by this operator to form a new path to the goal and, consequently, new cost-to-goal estimates. We can update the corresponding cost-to-goal estimate to approximate \hstar if it produces a shorter path than the current one. Note that although it is possible to expand the technique to neighbors of two or more distance operators, the computational cost grows exponentially with the average number of successors per sampled state.

% \begin{algorithm}[tb]
%     \caption{SUI algorithm}
%     \label{alg:sui}

%     \SetAlgoLined
%     \SetKwData{S}{S}
%     \SetKwData{s}{s}
%     \SetKwData{sp}{s'}
%     \SetKwData{su}{s'}
%     \SetKwData{t}{t}
%     \SetKwData{Ms}{A}
%     \SetKwData{Mh}{V}

%     \KwData{
%         \S: the sample set
%     }
%     \KwResult{\S: the updated sample set}
%     \SetKwProg{Fn}{Function}{:}{}

%     \SetKwFunction{SUI}{SUI}
%     \Fn{\SUI{\S}}{
%         \tcp{Let $h$ be the mapping from sampled states to cost-to-goal estimates.}
%         $\Mh \gets \emptyset$ \tcp*[h]{mapping from state to integer} \\
%         $\Ms \gets \emptyset$ \tcp*[h]{mapping from state to list of states} \\
%         \ForEach{$\s \in \S$}{
%             \If{$\s \notin \Ms$}{
%                 \ForEach{$\sp \in \sucs(\s)$}{
%                     $\Ms[\s] \gets \Ms[\s] \cup \{\t \mid \t \in \S\ \textnormal{\textbf{and}}\ \t \subseteq \sp\}$
%                 }
%             }
%             $\Mh[\s] \gets \min(\Mh[\s], h[\s])$ \\
%         }

%         \Repeat{no \h-value update}{
%             \ForEach{$\s \in \Ms$}{
%                 \ForEach{$\t \in \Ms[\s]$}{
%                     \If{$\Mh[\s] > \Mh[\t] + 1$}{
%                         $\Mh[\s] \gets \Mh[\t] + 1$ \\
%                         $h[\s] \gets \Mh[\s]$
%                     }
%                 }
%             }
%         }

%         \Return \S \\
%     }
% \end{algorithm}

% The method is detailed in \cref{alg:sui}. 
Consider a directed, weighted, and labeled graph~$G=(V,A)$ where each vertex in $V$ corresponds to a sampled state and is labeled by its lowest cost-to-goal estimate. We insert an arc~$(s,t)$ into $A$ for every pair of states~$s,t \in V$ such that $t \in \sucs(s)$. When generating a successor of a partial state, we typically generate another partial state. Since a partial state represents a set of complete states, to determine if a successor~$s' \in \sucs(s)$ matches any sampled state~$t$, we need to compute the subsets of $t$. For this, we maintain a trie data structure, where each sampled state is inserted with its variable values as key. When generating a successor~$s'$, we search the trie for sampled states~$t$ supersets of $s'$; that is, $s$ generates $t$ from applying an operator $o$ and assigning zero or more undefined variables. Thus, we can include an arc $(s, t)$ in~$A$ with a weight~$c_{s,t} = \text{cost}(o)$.

Using the graph $G$, we propagate the cost-to-goal estimate from each sampled state to its sampled predecessors. We iterate over each arc $(s,t) \in A$ and update the cost-to-goal estimate $h(s) = \min(h(s), h(t)+c_{s,t})$. The process continues as long as there are updates. For partial states generated by regression, by construction, at least one successor exists, except for the goal~$s^*$. Therefore, in the worst case, the algorithm will make $L$ iterations, propagating the cost-to-goal estimate of a goal state to the most distant state. As for SAI, all distances are witnessed by plans, so \cref{prop:hvalue} is maintained.

\begin{figure}[tb]
    \caption[SUI technique applied to samples of a random walk rollout.]{SUI technique applied on samples of a random walk rollout (red arches). Each node represents a state, and each arc is an applicable operator.}
    \label{fig:sui}
    \addmargin
    \centering
    \input{figures/sui}
\end{figure}

\cref{fig:sui} shows an example of the technique. The rollout does not overlap but intersects at a distance of one operator between states $s_1$ and $s_3$. This information is discovered by inserting the black arcs into the graph~$G$, which are operators that were not previously used by the sampling. From there, we can connect these states to generate a new path from $s_3$ to the goal that is shorter than the one produced by the rollout, updating its cost-to-goal estimate from $4$ to $h(s_1)+1=1+1=2$, according to the new path. Now, $s_3$ can update the estimate of its neighbors in the next iteration. Thus, all states generated in the rollout after $s_3$ will be updated. Therefore, although distant states from the goal have fewer neighbors compared to those near the goal, their cost-to-goal estimates are updated through the rollout propagation.

\section{Workflow}
\label{sec:workflow}

\begin{figure}[tb]
    \caption{Sample generation workflow.}
    \label{fig:workflow}
    \addmargin
    \centering
    \input{figures/workflow}
\end{figure}

Our approach follows the workflow shown in \cref{fig:workflow}.
The workflow begins with the sampling phase, where a set of samples is generated using algorithms such as random walk or FSM. The sampling must be performed through regression to enable the SUI; however, there are no restrictions on the other techniques.
After obtaining the sample set, the first step of sample processing is to apply SAI to the samples, represented as partial states at this stage. Then, the SUI technique is performed, which achieves the same outcome even if SAI is not applied to the partial states. However, applying SAI beforehand is computationally advantageous -- except in cases where the effect of the SAI is minimal -- as it reduces the number of iterations required to update the cost-to-goal estimates in the SUI stage.
The SUI is the last step, where the samples are treated as partial states. Next, the undefined values of each sample are assigned according to the chosen state completion technique, resulting in a sample set of complete states.
Then, we use random sampling to generate complete states for the sample set. By generating the random samples before applying SAI to complete states, there is no need to check if each randomly generated sample is already in the sample set to copy its cost-to-goal estimate, since a high cost-to-goal estimate $L+1$ is reduced by the SAI in this case. This step is handled by SAI, which updates the value if necessary. Applying SAI to the complete states concludes the preprocessing of the sample set before it proceeds to the supervised learning stage.

Overall, this workflow applies a series of transformations to the sample set. Each step contributes to adjusting the distribution of samples over the state space or refining their cost-to-goal estimates, both key factors for enhancing the quality of the sample set. Finally, the preprocessed sample set is used in the supervised learning phase, independent of this workflow, to train a heuristic function.

\begin{figure}[tb]
    \caption[An example of sampling in Blocks World.]{An example of sampling in Blocks World. Each set of blocks corresponds to a state, with its facts described on the side and its label below.}
    \label{fig:example}
    \addmargin
    \centering
    \input{figures/bw-example}
\end{figure}

Examples of all techniques can be extracted from \cref{fig:example}. The subset of the state space presented consists of $6$~states from the Blocks~world with $3$ blocks. Each state has a set of facts that describe its configuration. These facts include $\factclear{A}$ to indicate that block~$A$ has no block above it, $\facton{A}{B}$ to denote that block~$A$ is placed on top of block~$B$, and $\factontable{A}$ to signify that block~$A$ is on the table. The block labels are abbreviated based on their colors, with red, blue, and green represented as~$R$, $B$, and $G$, respectively. In this context, the goal state is defined as a stack of all blocks, with the blue block on the table and the red block on top. The described facts are based on applying backward operators from the goal state.

We can illustrate a regression using the FSM algorithm. Suppose we want to sample $6$~states with $p_{\bfsrw}=0.67$ and a maximum rollout limit of $L=4$. The goal state is initially sampled and expanded, adding the state~$s_1$ to the open queue $Q$ (see \cref{alg:fsm}). In the second iteration, $s_1$ is expanded, adding $s_2$ and $s_3$ to $Q$, which are also expanded and sampled in the next iteration. The BFS phase ends with $4$~samples, and the random walk phase begins, where $s_2$ and $s_3$ are the starting states for rollouts. Consider $s_3$ for the first rollout, which generates $s_4$ and then $s_5$. The starting state $s_3$ was sampled in the BFS and is not sampled again, so only $s_4$ and $s_5$ are inserted in the sample set. Upon reaching $s_5$, the rollout ends due to the maximum regression limit~$L = 4$. The sample budget is reached, and the sample generation ends. If the sample budget were larger, a new rollout would start from a predecessor of $s_2$ or $s_3$ excluding $s_4$. Assuming unit costs, the cost-to-goal estimate of a sample is the number of hops to the goal, e.g.,~$h(s_4)=3$.

Originally, a random walk rollout cannot visit states sampled by BFS. However, even though $s_4$ has the same configuration as~$s_2$, they have different facts, making them distinct states according to duplicate detection. Note that $h(s_2)=2$ and $h(s_4)=4$, i.e.,~the cost-to-goal estimate of $s_4$ overestimates \hstar. When using the mutex-based approach to complete states, both $s_2$ and $s_4$ become the same complete state~$s = \{$\mbox{\factclear{R}}, \mbox{\factclear{G}}, \mbox{\factclear{B}}, \mbox{\factontable{R}}, \mbox{\factontable{G}}, \mbox{\factontable{B}}$\}$, and the SAI updates the cost-to-goal estimate $h(s_4)=\min(4,2)=2$. On the other hand, SUI also adjusts the value. When creating the graph~$G$, a new arc is inserted from $s_4$ to $s_1$ motivated by the operator that moves the green block onto the blue block. Thus, we can update the estimate of $s_4$ to $h(s_4)=h(s_1)+1=1+1=2$, reaching \hstar in the same way. In the next iteration, the cost-to-goal estimate of $s_5$ is updated to $h(s_5)=h(s_4)+1=2+1=3$, also reaching~\hstar.
