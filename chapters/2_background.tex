\chapter{Background}\label{sec:preliminaries}

This chapter provides an overview of the fundamental concepts and techniques that establish the background for this dissertation. In Section~\ref{sec:background_classicalplanning}, we introduce classical planning and its notation, and present the STRIPS representation (Section~\ref{sec:background_strips}) used for modeling planning tasks, as well as the finite domain representation (Section~\ref{sec:background_sas}), which allows for more expressive problem formulations and composes the search framework detailed in Section~\ref{sec:background_searchframework}. We introduce heuristic search in Section~\ref{sec:background_heuristicsearch}, an approach that utilizes heuristic functions to guide the search to solve planning tasks. The chapter also discusses heuristic functions in Section~\ref{sec:background_heuristicfunctions}. Additionally, Section~\ref{sec:background_serchalgorithms} provides a detailed explanation of common search algorithms used in sample generation. Neural networks, trained to learn heuristic functions, are discussed in Section~\ref{sec:background_neuralnetworks}, and our specific architecture model is presented in Section~\ref{sec:background_resnets}. Finally, the chapter reviews related work (Section~\ref{sec:background_relatedwork}), discussing previous approaches and techniques that have contributed to sample generation and learning of heuristic function.

\section{Classical Planning}
\label{sec:background_classicalplanning}

To find a solution from search algorithms, it is necessary to provide a formal description of the problem. In classical planning, a problem is represented as a planning task. We define the various concepts that constitute a planning task, which are addressed throughout the dissertation.

\begin{definition}[Variable]\label{def:variable}
    A variable $V$ has a finite domain $D(V)$ and is defined as a single value $v \in D(V)$.
\end{definition}

The variables define the current state of the environment. For example, consider a domain with a robot that navigates on a grid. One variable, $V_{at}$, represents the robot's position, where each value $v \in D(V_{at})$ corresponds to a location on the grid. Typically a domain has several variables.

\begin{definition}[Fact]\label{def:fact}
    An fact $f$ is an assignemnt of a value $v \in D(V)$ to a variable $V$.
\end{definition}

Each variable generates $|D(V)|$ facts. When considering the previous example in a $2 \times 2$ grid with 4 tiles labeled $t_0$ to $t_3$, the variable $V_{at}$ has the set of facts $F_{at}=\{\text{at}(t_0),\ldots,\text{at}(t_3)\}$.

\begin{definition}[Mutex]\label{def:mutex}
    A mutex (mutual exclusion) is a condition where two or more facts can not occur simultaneously.
\end{definition}

In the example domain, the robot cannot be in two positions simultaneously. Hence, any two facts from the set $F_{at}$ represent a mutex. By definition, a variable assumes a single value $v \in D(V)$, and therefore, the variable combines facts that are mutexes. However, mutex conditions can also arise between values of different variables.

\begin{definition}[State]\label{def:state}
    A state $s$ is an assignment of all variables $V \in \mathcal{V}$ where $\mathcal{V}$ is the set of variables of a task.
\end{definition}

A state in which all variables are defined is also called a complete state. When the set of variables is not fully assigned, i.e., one or more variables $V \in \mathcal{V}$ do not have a defined value $v \in D(V)$, it is a partial state. Let $s(V)$ be the value of variable $V$ in state $s$. The value of an undefined variable $V$ is detoned by the symbol $\bot$ and can assume any value $v \in D(V)$. We say that $s \subseteq t$ if the set of facts of state $s$ is completely contained in the set of facts of state $t$, then it is possible to assign the undefined variables such that $s = t$. Therefore, a partial state $p$ is a set of states containing every state whose $s \subseteq p$. The initial state $s_0$ is a complete state that corresponds to the initial variable assignment. The goal state $s^*$ is defined as a partial state.

\begin{definition}[Operator]\label{def:operator}
    An operator $o$ is defined as a pair of preconditions and effects $(\pre(o),\eff(o))$, both partial states. The preconditions specify the conditions that must hold true in the current state for an operator to be applicable, while the effects describe the changes in the state that occur when the operators is applied.
\end{definition}

An operator $o$ is applicable to a state $s$ if $\pre(o) \subseteq s$, and produces a successor state $s'=\sucs(s,o):=\eff(o)\circ s$, where $s'=t\circ s$ is defined by $s'(v)=t(v)$ for all $v$ such that $t(v)$ is defined, and $s'(v)=s(v)$ otherwise. The set of all successor states of state $s$ is~$\sucs(s)=\{\sucs(s,o)\mid o\in \mathcal{O}, \text{o applicable to }s\}$. If $s$ is a partial state, when $\pre(o)$ mentions a variable $v$ and $s(v) = \bot$ then it is applicable over $s$.

A sequential application of operators is called a progression. Alternatively, a regression is a backward sequential application of operators. For regression, we consider an operator $o$ to be relevant for partial state~$s$ if $\eff_r=\dom(\eff(o))\cap\dom(s)\neq\emptyset$; the operator is consistent if $\eff(o)|_{\eff_r} \subseteq s$. Relevance requires at least one defined effect in the partial state to be regressed, consistency and an agreement on defined effects. An operator~$o$ then is \define{backward} applicable in partial state~$s$ if it is relevant and consistent with~$s$ and leads to predecessor $r=\pre(o)\circ (s|_{\dom(s)\setminus\eff_r})$. Note that $\sucs(r,o)\subseteq s$, but may differ from $s$. Similar to progression, a partial state~$s$ has predecessors $\pred(s)=\{\pred(s,o)\mid o\in \mathcal{O}, \text{o backward applicable to }s\}$. A regression sequence from state $s_0$ then is valid if $o_i$ can be applied to $s_{i-1}$ and produces $s_i=\pred(s_{i-1},o_i)$. All partial states~$s_k$ can reach a partial state $s\subseteq s_0$ in at most~$k$ forward applications of the reversed operator sequence.

\begin{definition}[Planning Task]\label{def:planningtask}
    A~\sas planning task is a tuple $\Pi=\langle\mathcal{V},\mathcal{O},s_0,s^*, \text{cost}\rangle$, where $\mathcal{V}$~is a set of variables, $\mathcal{O}$~is a set of operators, $s_0$~an initial state, $s^*$ a goal condition, and $\text{cost}:\mathcal{O}\rightarrow\R_{+}$ a function mapping operators to costs.
\end{definition}

The following sections cover the main formalisms for representing planning tasks.

\begin{definition}[Plan]\label{def:plan}
    A plan is a sequence of operators $\pi=(o_1,\ldots,o_k)$, $o_i\in \mathcal{O}$ with $o_i \in \mathcal{O}$.
\end{definition}

A plan is valid for state~$s_0$ ($s_0$-plan), if for $i\in[k]$ operator~$o_i$ can be applied to $s_{i-1}$ and produces $s_i=\sucs(s_{i-1},o_i)$, where $s_k \in s^*$. The cost of plan~$\pi$ is $\sum_{i\in[k]} \text{cost}(o_i)$. When a plan has the lowest cost among all $s_0$-plans it is called an optimal plan.

\begin{definition}[State Space]\label{def:statespace}
    A state space is the set of all states over $\mathcal{V}$.
\end{definition}

The forward state space (\fssp) is the set of all states reachable from the initial state~$s_0$ by applying a sequence of operators. Similarly, the backward state space (\bssp) is the set of all partial states reachable from the goal~$s^*$ by applying a sequence of backward applicable operators.

\subsection{Propositional Representation}
\label{sec:background_strips}

Propositional representation is a fundamental approach used in planning tasks to model and reason about the state of the environment. In this representation, a planning task is defined by a set of propositional variables (facts) that describe the various attributes and conditions of the problem domain. Each predicate represents a specific property that can be true or false in a given state. Additionally there is the set of operators, also known as actions, that can be applied to transition between states. The operators are expressed using the predicates. 

The propositional representation, with its predicates and actions, forms the basis for the STRIPS (Stanford Research Institute Problem Solver) representation, one of the most widely used models for planning tasks. STRIPS extends the propositional representation by introducing formalized syntax and semantics for representing preconditions and effects of operators, enabling automated planning algorithms to reason about the state transitions and generate plans.

Throughout this dissertation, samples composed of states in the STRIPS representation are presented and utilized. The motivation behind this approach lies in the compatibility between the propositional nature of STRIPS, where facts can be either true or false, and the proposed neural network input in binary format which is more suitable for training (Section~\ref{sec:background_learningheuristics}). Additionally, we use the Fast~Downward, presented in Section~\ref{sec:background_searchframework}, that incorporates PDDL (Planning Domain Definition Language) as its input language. PDDL provides a formal and widely accepted syntax for describing planning tasks in the predicate logic, compatible with STRIPS representation.

\begin{figure}[ht]
\caption{VisitAll domain description in PDDL.}
\label{fig:pddl}
\addvspace{\baselineskip}
\centering
\begin{lstlisting}[basicstyle=\ttfamily]
(define
    (domain grid-visit-all)
    (:requirements :typing)
    (:types place - object)
    (:predicates
        (connected ?x ?y - place)
        (at-robot ?x - place)
        (visited ?x - place)
    )
    (:action move
        :parameters (?curpos ?nextpos - place)
        :precondition (and
            (at-robot ?curpos)
            (connected ?curpos ?nextpos)
        )
        :effect (and 
            (at-robot ?nextpos)
            (not (at-robot ?curpos))
            (visited ?nextpos)
        )
    )
)
\end{lstlisting}
Source: International Planning Competition 2014.
\end{figure}

Figure~\ref{fig:pddl} shows an example of a domain description in PDDL. The VisitAll domain represents a scenario where a robot navigates a grid and marks each tile it steps on as visited. The objective of this domain is typically to visit all tiles in the grid. The initial state and the goal state are described in the problem PDDL, along with the object declarations. The domain PDDL specifies the predicates and actions (operators). In VisitAll example, we have one object type (place), three predicates (connected, at-robot, and visited), and one action (move). The combination of predicates with objects forms facts. For instance, in a grid with two connected tiles, $t1$ and $t2$, generates the set of facts \{connected$(t1,t2)$, connected$(t2,t1)$, at-robot$(t1)$, at-robot$(t2)$, visited$(t1)$, visited$(t2)$\}.

\subsection{Finite Domain Representation}
\label{sec:background_sas}

% \cite{Backstrom.Nebel/1995}

\subsection{Search Framework}
\label{sec:background_searchframework}

\section{Heuristic Search}
\label{sec:background_heuristicsearch}

\subsection{Heuristic Functions}
\label{sec:background_heuristicfunctions}

% Ideally, we would like to learn in a model-free setting where we interact with the planning task only by functions that allow accessing the initial state~$s_0$, the goal condition~$s^*$, and the applicable operators in a partial state. In this setting, we do not have access to the logical description of operators, we only have access to black-box functions~\cite{Sturtevant2019} -- which could also be learned -- that, given a partial state, returns its successors and predecessors. Note that this setting is also used in reinforcement learning. Some approaches also provide access to each variable's domain. In contrast, a model-based heuristic uses the complete description of the model, which permits, for example, reasoning about operators and the computation of mutexes. Finally, we consider a heuristic strongly model-based if it uses complete planners to generate the samples, including search algorithms and logic-based heuristic functions.

\section{Search Algorithms}
\label{sec:background_serchalgorithms}

\section{Neural Networks}
\label{sec:background_neuralnetworks}

\subsection{Residual Networks}
\label{sec:background_resnets}

\section{Learning Heuristic Functions with Neural Networks}
\label{sec:background_learningheuristics}

Many heuristics for classical planning are derived from a model of the task, such as the \sas model introduced in this chapter. An obvious alternative is to learn to map a state $s$ to its heuristic value $h(s)$. We focus here on learning with NNs, although other supervised learning methods could be used. To learn a heuristic function, an NN is trained on pairs of states~$s$ and cost-to-goal estimates~$c$. The learned heuristic functions are usually not admissible, so traditional optimality guarantees are lost.

A propositional representation of a state is more suitable for learning functions over states. To this end, consider a planning task $\Pi=\langle\mathcal{V},\mathcal{O},s_0,s^*, \text{cost}\rangle$, and let $\mathcal{V}=\{v_1,\ldots,v_n\}$ and $D(v_i)=\{d_{i1},\ldots,d_{i,s_i}\}$, $i\in[n]$ be some order of the variables and their domains. We represent any state $s$ by a sequence of facts $$\mathcal{F}(s)=(f_{11},f_{12},\ldots,f_{1,s_1},\ldots,f_{n1},f_{n2},\ldots,f_{n,s_n}),$$ where each fact $f_{ij}=[s(v_i)=d_{ij}]$ indicates if variable $v_i$ assumes value $d_{ij}$ in state $s$. Note that facts $\mathcal{F}_i=\{f_{i1},\ldots,f_{i,s_i}\}$ corresponding to variable $v_i$ satisfy the consistency condition $\sum_{f\in \mathcal{F}_i} f\leq 1$ since each variable assumes at most one value, and $\sum_{f\in \mathcal{F}_i} f=0$ only if $v_i$ is undefined. More generally, for any set of facts $\mathcal{F}$ we write $\mutex(\mathcal{F})$ if $\sum_{f\in \mathcal{F}} [f]\leq 1$ must be satisfied in states of $\Pi$. Many planning systems can deduce mutexes from the description of the planning task $\Pi$~\cite{Helmert/2009}; we will discuss and analyze their utility for sampling states later. Some architectures provide additional input to the NN, e.g.,~the propositional representation of the goal condition. The target output for training may be the cost-to-goal estimates directly or some encoding of them.

An important aspect of sample generation related to challenges C1 and C2 (Section \ref{sec:intro}) is the degree of dependency on the domain model -- ideally, we would like to learn in a model-free setting -- or the planning task and the cost of generating the samples. The cost of sample generation depends on the number of samples and the cost to generate each. This generates the problem of deciding how many samples are required, although in general, only a very small part of the state space can be sampled. More importantly, ideal samples would be labeled with the perfect heuristic $h^*$, which maps each state~$s$ to the cost of an optimal $s$-plan or $\infty$ if no such plan exists. In general, ideal labeling is impractical since it requires solving the planning task on a large number of initial states. Therefore, we are mainly interested in good heuristic estimates that can be generated fast. We analyze the influence of sample size and quality experimentally later.

Additionally, and related to challenges C3 and C4, network architecture and sample generation depends on the range of tasks the learner intends to generalize. This may be the state space of a planning task, a planning domain, or an entire planning formalism. In the first case, the set of planning tasks is defined over any pair of initial state $s_0$ and goal $s^*$. Often the set of planning tasks is restricted to select the initial state from the \fssp of some given initial state and to a fixed goal. In the second case, the learned function has to generalize over all domain tasks. Finally, a learning-based heuristic that generalizes over a planning formalism is domain-independent. An important aspect of sample generation is the distribution of states part of the sample set. For example, the sample set can contain only states with a short distance to the goal or only states with a short distance to the initial states. In this dissertation, the distribution of states describes how often each \hstar-value occurs in the sample set.

\section{Related Work}
\label{sec:background_relatedwork}

\rvi{WIP}

There have been two main research foci on learning heuristic functions. The first uses strongly model-based approaches to generate samples and aims to generalize over domains or a planning formalism. The second uses partially model-based approaches to generate samples and aims to generalize only over a state space. We describe these approaches as partially model-based because they only use the model to identify mutexes.

The usual setting for the first set of approaches~\cite{Toyer.etal/2018,Toyer.etal/2020,Shen.etal/2020,Gehring.etal/2022,Stahlberg.etal/2022} is to train a structured NN with samples of small tasks of a domain generated with a strongly model-based method and evaluated on larger tasks of the same domain.  
The structured networks trained can be general networks such as neural logic machines~\cite{Dong.etal/2018} and graph neural networks~\cite{Gori.etal/2005,Scarselli.etal/2008}, or networks proposed explicitly in the context of planning such as hypergraph neural network~\cite{Shen.etal/2020} and Action Schema Networks~\cite{Toyer.etal/2018}.
These networks require the logical description of the domain and the task to be instantiated and can generalize well, with competitive results compared to logic-based heuristics.
These approaches also help in understanding learning heuristics. For example, the main goal of \citeyear{Stahlberg.etal/2022} is to understand the expressive power and limitations of learning heuristics.
The main limitation of these approaches is the strong dependence on the domain model and task description.
  
The second set of approaches~\cite{Ferber.etal/2020a, Yu.etal/2020, Ferber.etal/2022, OToole/2022} typically trains an FNN and evaluates the learned heuristic on a state space using tasks with the same goal and different initial states. 
These networks are trained with pairs of states and cost-to-goal estimates. 
In this setting, \citeyear{Ferber.etal/2020a} systematically study hyperparameters on the architecture of the FNN and found that their influence is secondary. They found that for a fixed architecture, two aspects significantly influence how informed the heuristic is: the subset of selected samples and the size of the sample set. 

Furthermore, \citeyear{Yu.etal/2020} and \citeyear{OToole/2022} perform backward searches from the goal, the former with depth-first search and the latter with random walks. Both use the lowest depth in which the state was generated as cost-to-goal estimates. \citeyear{Ferber.etal/2022} uses a combination of backward and forward searches~\cite{Arfaee.etal/2011}. First, it generates new initial states with backward random walks and then solves them with a GBFS guided by a learned heuristic. The plans found provide the samples, and each sample is a state in the plan with the cost-to-goal estimate as its distance to the goal.
\citeyear{OToole/2022} also presented a method to generate samples that do not use expansions. This method includes randomly generated states in the sample set with high cost-to-goal estimates.
\citeyear{OToole/2022} showed that this method substantially increases coverage. 
The methods from the second set of approaches are highly independent of the domain model and planning task description and require low computational resources to generate samples and train the FNN. However, they suffer from high performance variability, given the FNN initialization and the sample set used. Also, despite having competitive results compared to logic-based heuristics, they are still unable to surpass the goal-count heuristic.
