\chapter{Limitations}
\label{sec:limitations}

In this section, we discuss common limitations and problems of methods based on learning heuristic functions with NNs.

An explicit limitation is that the validation loss of a trained model can be unreliable: for two NNs trained with the same configuration, it is not uncommon for the one with a larger loss to expand fewer states than the network with a smaller loss. For example, when training with \hstar-values over states generated with \bfsrw, with one sample seed and $100$ network seeds for each of the small state spaces, $36\,\%$ of the models with the least amount of expanded states had a higher validation loss than other models with more expanded states; if the states do not have \hstar-values, it increases to $49\,\%$. Regarding coverage, in the large state space experiments with the heuristic \hnnrs and $3$ sample seeds and $3$ network seeds ($9$ runs) for each state space, $15\,\%$ of the models with the highest coverage had a higher validation loss than the other models with lower coverage. This happens because the sampling procedure is imperfect -- an NN learns a limited portion of the state space, which varies per sample seed, so even if the validation loss is small, it provides no guarantees of search quality.

Also, as noted by other methods that use some form of model validation \cite{Ferber.etal/2020a, Shen.etal/2020, Ferber.etal/2022, OToole/2022}, training can be noisy, with multiple runs leading to very distinct results. A consequence of this is observed in our experiments comparing expanded states, where even a single state that ends up expanded due to an inaccurate heuristic can lead to numerous extra expansions, while other states can lead the search to a more direct path to the goal~\cite{Heusner.etal/2017}. Note that this problem is common for all approaches that use \gbfs, but this is further aggravated in NNs that learn from approximated values.
An example is shown in Table~\ref{tab:small-baseline-best-stdev} comparing the mean number of expanded states of the baseline and \hnnrs. We see that standard deviation varies per domain (i.e., some domains are noisier than others), and training with better-quality samples typically helps. Regarding coverage in large state spaces, the standard deviation was smaller, as it does not vary at the same rate as expanded states. For example, \hnnrs has a standard deviation of $16\,\%$ in Grid coverage, while the mean standard deviation considering all the other domains is less than $3\,\%$.

\input{tables/small-baseline-best-stdev}

Furthermore, this work and others \cite{Ferber.etal/2020a, Ferber.etal/2022, OToole/2022} use similar NN architectures with fixed input format, namely a boolean representation of a sequence of facts, where each one is represented as a neuron. However, this is inefficient when training over large planning tasks that can have an input size in the order of thousands. As far as we know, there is currently no alternative for FNNs; graph-based neural networks such as in \citeyear{Shen.etal/2020} represent the input states as graphs.
