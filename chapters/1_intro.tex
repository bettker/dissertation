\chapter{Introduction}
\label{sec:intro}

Classical planning provides a method for representing and solving various problems. By formulating these problems as planning tasks, it can effectively model real-world challenges such as resource allocation, scheduling, robotics, and autonomous systems. This approach enables automated systems to reason, make decisions, and generate plans to achieve specific objectives. The ability to represent and solve problems using classical planning techniques has garnered significant attention and has proven instrumental in various domains.

Planning tasks are typically defined by an initial state, which represents the starting configuration of the system, and a goal state, which represents the desired outcome. States capture the relevant information about the system's state at a particular time. Each domain provides a set of operators that describe how the state can be transformed by performing a specific action. By applying a sequence of operators, a plan is constructed to reach the goal state from the initial state. The plan provides a step-by-step guide for an agent or a system to follow to achieve the desired objective. By employing various planning algorithms and techniques, intelligent systems can efficiently explore the space of possible actions and generate optimal or near-optimal plans to solve complex problems.

Several approaches can find a sequence of operations that transforms an initial state into one that satisfies a goal condition. Any such plan is satisficing, but often we require plans to be optimal because the total cost of all operations is minimal. One successful strategy for solving such tasks is to apply algorithms of the best-first search family, which use a function~$f$ to guide the search. For a given state~$s$, the function~$f$ can combine the current cost~$g(s)$ of a partial sequence of operations starting in the initial state and resulting in~$s$, and a heuristic function~$h(s)$ that estimates the cost-to-goal from state~$s$. For example, \astar search is guided by $f(s)=g(s)+h(s)$~\cite{hart-et-al-ieeessc1968}, and greedy best-first search is guided only by $f(s)=h(s)$~\cite{doran-michie-rsl1966}. Generally, best-first search algorithms are more effective when the heuristic function~$h$ better estimates the optimal cost-to-goal.

Logic-based relaxations of planning tasks create some of the most successful heuristic functions~$h$, e.g.,~the delete relaxation~\cite{Hoffmann.Nebel/2001}, landmarks~\cite{hoffmann-et-al-jair2004,Karpas.Domshlak/2009}, critical paths~\cite{haslum-geffner-aips2000}, or the state equation~\cite{bonet-ijcai2013}. Many of these heuristics come with additional properties, such as admissibility. A heuristic is admissible if it does not overestimate the optimal cost-to-goal. \astar guided by admissible heuristic is guaranteed to find an optimal plan, if one exists.

Driven by rapid progress in other application areas, interest in learning heuristic functions with neural networks has been renewed~\cite{samadi-et-al-aaai2008,Arfaee.etal/2011,Agostinelli.etal/2019,Yu.etal/2020,Shen.etal/2020,Ferber.etal/2020a,Toyer.etal/2020,Ferber.etal/2022,OToole/2022}. The basic approach is simple: one generates a set of samples of pairs of states and estimates of cost-to-goal and trains a supervised model over the set of samples. However, a successful approach to planning has to solve several challenges:

\begin{enumerate}[label=C\arabic*),left=0pt]
    \itemsep0pt
    \item State spaces are implicitly defined and mostly exponential in the size of a compact description. Therefore, random samples are hard to generate and may be infeasible, unreachable from an initial state, or unable to reach the goal. Samples are usually generated by expanding the state space through forward or backward search (regression).
    \item Estimates of cost-to-goal are typically hard to obtain. Finding the optimal cost-to-goal amounts to solving the task on the samples, and often the learned function is only useful if it is close to the optimal cost-to-goal.
    \item Planning domains are very different, and logic-based heuristics apply to any domain. This results in the problem of transferring a learned heuristic to new domains, new state spaces of a domain, or new states.
    \item Planners evaluate many states per second, so computing the heuristic function is often fast. Therefore, the learned heuristic must be more informed, or the complexity of the model must be restricted.
\end{enumerate}

By addressing these core issues, new possibilities can be unlocked for advancing the field of classical planning and enabling more efficient and effective problem-solving capabilities in intelligent systems.  For this, we investigate current state sampling methods and propose new techniques to address key problems that directly affect the quality of samples for training heuristic functions.

\section{Contributions}
\label{sec:intro_contributions}

Through controlled experiments on planning tasks with small state spaces, we identify several techniques that improve the quality of the samples used for training. Our contributions include the following:

\begin{itemize}
    \item A sample generation algorithm that can better represent a relevant subset of the state space through a combination of breadth-first search (exploring states close to the goal) followed by random walks from the \bfs's leaves (Section~\ref{sec:sampling-generation}).
    \item On-the-fly states space-based estimations to limit each sampling regression depth to avoid big cost-to-goal overestimates (Section~\ref{sec:rollout-depth-limit}).
    \item Two methods to improve cost-to-goal estimates based on detecting samples from the same or neighboring states. (Section~\ref{sec:hvalue}).
    \item A systematic study on sampling quality (Chapter~\ref{sec:experiments}).
\end{itemize}

\section{Overview and outline}
\label{sec:intro_outline}

This dissertation addresses strategies for generating samples which we introduce in Chapter~\ref{sec:sampling}, and in particular, what characterizes samples that lead to heuristic functions that perform well. To this end, Chapter~\ref{sec:experiments} presents a systematic study of the contributions of each strategy when solving distinct initial states of a single state space. We learn state-space-specific heuristics using a feedforward neural network, focusing mainly on the quality of the learned heuristic and its influence on the number of expanded states and coverage. We aim to understand better how to learn high-quality heuristics. In experiments on small state spaces in Section~\ref{sec:experiment1}, we investigate the effect of different sampling strategies, the quality of the learned heuristic with an increasing number of samples, and the effect of a different subset of states part of the sample set on the learned heuristic; we also evaluate how the quality of the estimates of cost-to-goal influences the effectiveness of learned heuristic to guide a search algorithm. Then, in Section~\ref{sec:experiment2} we compare our best strategy with a baseline and traditional logic-based methods over large state spaces. Furthermore, we qualitatively compare existing methods in the literature in Chapter~\ref{sec:comparison} and discuss the limitations of learned heuristics in Chapter~\ref{sec:limitations}. Finally, we conclude and highlight possible future work in Chapter~\ref{sec:conclusion}.
