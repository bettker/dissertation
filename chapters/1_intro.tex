\chapter{Introduction}
\label{sec:intro}

Planning is finding a sequence of operations that transforms an initial state into one that satisfies a goal condition. Any such sequence, or plan, is satisficing, but often we require plans to be optimal because the total cost of all operations is minimal. One successful strategy for solving such tasks is to apply algorithms of the best-first search family, which use a function~$f$ to guide the search. For a given state~$s$, the function~$f$ can combine the current cost~$g(s)$ of a partial sequence of operations starting in the initial state and resulting in~$s$, and a heuristic function~$h(s)$ that estimates the cost-to-goal from state~$s$. For example, \astar search is guided by $f(s)=g(s)+h(s)$~\cite{hart-et-al-ieeessc1968}, and greedy best-first search (\gbfs) is guided only by $f(s)=h(s)$~\cite{doran-michie-rsl1966}. Generally, best-first search algorithms are more effective when the heuristic function~$h$ better estimates the optimal cost-to-goal.

Logic-based relaxations of planning tasks create some of the most successful heuristic functions~$h$, e.g.,~the delete relaxation~\cite{Hoffmann.Nebel/2001}, landmarks~\cite{hoffmann-et-al-jair2004,Karpas.Domshlak/2009}, critical paths~\cite{haslum-geffner-aips2000}, or the state equation~\cite{bonet-ijcai2013}. Many of these heuristics come with additional properties, such as admissibility. A heuristic is admissible if it does not overestimate the optimal cost-to-goal. \astar guided by admissible heuristic is guaranteed to find an optimal plan (if one exists).

Driven by rapid progress in other application areas, interest in learning heuristic functions with \define{neural networks}~(NNs) has been renewed~\cite{samadi-et-al-aaai2008,Arfaee.etal/2011,Agostinelli.etal/2019,Ferber.etal/2020a,Shen.etal/2020,Yu.etal/2020,Toyer.etal/2020,Ferber.etal/2022,OToole/2022}. The basic approach is simple: one generates a set of samples of pairs of states and estimates of cost-to-goal and trains a supervised model over the set of samples. However, a successful approach to planning has to solve several challenges:

\begin{enumerate}[label=C\arabic*),left=0pt]
    \itemsep0pt
    \item State spaces are implicitly defined and mostly exponential in the size of a compact description. Therefore, random samples are hard to generate and may be infeasible, unreachable from an initial state, or unable to reach the goal. Samples are usually generated by expanding the state space through forward or backward search (regression).
    \item Estimates of cost-to-goal are hard to obtain. Finding the optimal cost-to-goal amounts to solving the task on the samples, and often the learned function is only useful if it is close to the optimal cost-to-goal.
    \item Planning domains are very different, and logic-based heuristics apply to any domain. This results in the problem of transferring a learned heuristic to new domains, new state spaces of a domain, or new states.
    \item Planners evaluate many states per second, so computing the heuristic function is often fast. Therefore, the learned heuristic must be more informed, or the complexity of the model must be restricted.
\end{enumerate}

In this paper, we are interested in strategies for generating samples which we introduce in Section~\ref{sec:methods}, and in particular, what characterizes samples that lead to heuristic functions that perform well. To this end, Section~\ref{sec:experiments} presents a systematic study of the contributions of each strategy when solving distinct initial states of a single state space. 
We learn state-space-specific heuristics using a \define{feedforward neural network}~(FNN), focusing mainly on the quality of the learned heuristic and its influence on the number of expanded states and coverage. We aim to understand better how to learn high-quality heuristics. In experiments on small state spaces in Section~\ref{sec:experiment1}, we investigate the effect of different sampling strategies, the quality of the learned heuristic with an increasing number of samples, and the effect of a different subset of states part of the sample set on the learned heuristic; we also evaluate how the quality of the estimates of cost-to-goal influences the effectiveness of learned heuristic to guide a search algorithm. Then, in Section~\ref{sec:experiment2} we compare our best strategy with a baseline and traditional logic-based methods over large state spaces. Furthermore, we qualitatively compare existing methods in the literature in Section~\ref{sec:comparison-other-methods} and discuss the limitations of learned heuristics in Section~\ref{sec:limitations}. Finally, we conclude and highlight possible future work in Section~\ref{sec:conclusions}.

\section{Contributions}
\label{sec:contributions}

Through controlled experiments on planning tasks with small state spaces, we identify several techniques that improve the quality of the samples used for training. Our contributions include:

\begin{itemize}
    \item A sample generation algorithm that can better represent a relevant subset of the state space through a combination of breadth-first search (exploring states close to the goal) followed by random walks from the \bfs's leaves (Section~\ref{sec:sampling-generation}).
    \item On-the-fly states space-based estimations to limit each sampling regression depth to avoid big cost-to-goal overestimates (Section~\ref{sec:rollout-depth-limit}).
    \item Two methods to improve cost-to-goal estimates (Section~\ref{sec:hvalue}).
    \item A systematic study on sampling quality (Section~\ref{sec:experiments}).
\end{itemize}
