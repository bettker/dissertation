\chapter{Experiments}
\label{sec:experiments}

In this section, we present two sets of experiments. In the first set, we analyze the behavior of sampling methods on planning tasks for which we can enumerate the complete state space with associated perfect cost-to-goal estimates \hstar. We study how different techniques can influence the number of state expansions by a search algorithm. In the second set, we evaluate how our findings generalize to a practical setting with large planning tasks. Our methods are then compared to logic-based heuristics.

\section{Settings}
\label{sec:settings}

We use a residual neural network~\cite{he2016deep} to learn a heuristic for a state space. The network's input is a Boolean representation of the states, where a fact is set to~$1$ if it is true in the state and~$0$ otherwise, as explained in \cref{sec:learning-heuristics}, and its output is a single neuron with the predicted \h-value. The network has two hidden layers followed by a residual block with two hidden layers. Each hidden layer has~$250$ neurons that use ReLU activation and are initialized as proposed by \citet{he2015delving}. The training uses the Adam optimizer~\cite{kingma2015adam}, a learning rate of~$10^{-4}$, early-stop patience of~$100$, and an MSE loss function. Due to better results in preliminary experiments, we use batch sizes of~$64$ for small and~$512$ for large state spaces. We use $90\,\%$ of the sampled data as the training set, with the remaining $10\,\%$ as the validation set. In the experiments, different learned heuristics are denoted as~$\hat h_A$, where~$A$ indicates different algorithmic choices.

We select the domains and tasks from \citet{ferber2022neural}: Blocks, Depot, Grid, N-Puzzle, Pipes-NT, Rovers, Scanalyzer, Storage, Transport, and VisitAll. All domains have unit costs except for Scanalyzer and Transport, for which we consider the variant with unit costs. All methods are implemented on the Neural~Fast~Downward planning system with PyTorch~1.9.0~\cite{ferber2020neural,paszke2019pytorch}. Our source code, planning tasks, and experiments are available\footnote{Available at \url{https://github.com/bettker/NeuralFastDownward}}. All experiments were run on a PC with an AMD~Ryzen~9~3900X processor, using a single core with $4$\,GB RAM per process. We solve all tasks with \gbfs guided by the learned heuristic~\hnn with the lowest generation order as a tie-breaking strategy.

We note that an NN may fail to train if, after initialization, it outputs zero for all training samples~(this is called ``born dead'' in \citet{lu2020dying}). This occurs with a non-negligible frequency in experiments on smaller state spaces with fewer samples. Thus, we test for this condition. Suppose the network outputs zero for all samples in the training set after initialization. In that case, we reinitialize with a different seed until the network outputs a non-zero value for some sample.

We propose a baseline \hnnbase to establish a point of comparison for evaluating the performance of our approaches. The baseline refers to a NN configured similarly to previous partially model-based methods described in \cref{sec:related-work-partially}. The \hnnbase is trained using random walks with $L=200$. Mutexes are applied during the regression and for state completion, but resetting the \h-value to~$0$ in goal states and the improvement strategies SAI and SUI are turned off. By comparing the results of our approaches with the baseline, we can assess the potential improvements over existing methods.

To determine a value for $p_{\bfsrw}$, preliminary experiments in small state spaces were performed using $p_{\bfsrw}\in\{0.01,0.05,0.1,0.2,\ldots,0.9\}$. We use the same baseline configuration but with the FSM sampling algorithm. The corresponding geometric mean expansions obtained were $84.92$, $79.51$, $74.05$, $79.46$, $80.39$, $80.79$, $96.17$, $120.42$, $133.99$, $167.14$, and $175.58$, respectively. As a result, we set a fixed value of $p_{\bfsrw} = 0.1$ for the experiments.

\section{Small State Spaces}
\label{sec:small-experiments}

In this section, we study the behavior of different sampling methods on small state spaces. For each domain, we select the task from the IPC benchmarks with the largest size state space that can be enumerated completely to obtain \hstar-values. We only select tasks with state spaces with $30$\,K~states or more, and fewer than $1$\,M~states. \cref{tab:small-fss-size} shows the tasks and their state space sizes. For domains Grid, Rovers, Scanalyzer, and Transport, the best task found had fewer than $30$\,K states, and VisitAll more than $1$\,M, so we manually modified these tasks. We could not find a task within our limits for Depot, Pipes-NT and Storage, so they were excluded from our experiments. We generate the initial states for the small state spaces by performing a random walk of length~$200$ from the original initial state of a task. Rovers, Scanalyzer, and VisitAll had duplicated initial states or states that satisfied a goal state. Thus, we generate the initial states for these domains with random walks of length~$25$, $50$, and $8$, respectively.

\begin{table}[tb]
    \caption[Size of the forward state spaces for the selected domains.]{Size of the forward state spaces for the selected small tasks in seven domains. Tasks marked with~$*$ were modified.} 
    \label{tab:small-fss-size}
    \addmargin
    \centering
    \input{tables/small-fss-size}
\end{table}

In the small state space experiments, the coverage for all methods is $100\,\%$. Therefore we use the number of expanded states to evaluate the quality of the heuristic function. In these experiments, we report means over experiments with five different network seeds, five different sample seeds, and $50$~initial states. The training time has been limited to $30$~minutes. If not stated otherwise, methods \bfs, \dfs, \rw, and \bfsrw use mutexes, the improvement strategies SAI and SUI, and the number of samples is equal to $1\%$ of the state space size. After initialization, the percentage of networks that output~$0$ for all samples was at most $40\,\%$~(in Blocks), and all networks successfully passed the initialization test after the second initialization. Under these conditions, less than $1.5\,\%$ of the NNs did not converge within the time limit.

\subsection{Distribution of States}
\label{sec:small-exps-distribution}

Our first set of experiments aims to analyze the influence of the distribution of sampled states in the state space on the quality of the learned heuristics. We compare different sample generation algorithms, regression limits, random sample percentages, and state completion methods. If not stated otherwise, for the parameters that are not varied, we use the baseline setting defined above~(regression limit~$L=200$, mutexes, no cost-to-goal improvement methods).

\subsubsection{Sample Generation Algorithms}
\label{sec:small-exps-algorithm}

This experiment compares four sample generation algorithms: \bfs, \dfs, \rw, and \bfsrw. To control the effect of the cost-to-goal estimates on the quality of the learned heuristic, we replace sample estimates with perfect values~\hstar before training. \cref{tab:small-sampling-hstar} shows the number of expanded states of a \gbfs guided by the learned heuristics and the mean \hstar-values over the sampled states. We see that heuristic \hnnl{\bfs} leads to more expanded states than \hnnl{\dfs}, which in turn expands about $50\,\%$ more states than \hnnl{\rw} and \hnnl{\bfsrw}, which perform similarly. Using \hnnl{\bfs} is significantly worse and leads to the highest or close to the highest number of expansions in all domains. Heuristic \hnnl{\dfs} has a high number of expansions in Blocks, N-Puzzle, and Transport. Looking at the mean \hstar-values, we see that samples generated by \bfs have the lowest, and those by \dfs the highest mean estimates in all domains. Although the distribution of \dfs is closest to that of the whole state space~(inferred from the mean \hstar-values), the resulting heuristic expands more states than \rw and \bfsrw, which generate states closer to the goal. Therefore, multiple random walk rollouts seem better than a single rollout with \bfs or \dfs due to increased sample diversity in different portions of the state space, covering states more likely to be visited during the search.

\begin{table}[tb]
    \caption[Comparison of sampling strategies on \hstar-values.]{Comparison of sampling strategies~\bfs, \dfs, \rw, and \bfsrw on \hstar-values. Expanded states of \gbfs with learned heuristics and mean \hstar-values over the entire forward state space and the generated sample sets.}
    \label{tab:small-sampling-hstar}
    \addmargin
    \centering
    \input{tables/small-sampling-hstar}
\end{table}

We now compare these results to results shown in \cref{tab:small-sampling-h}, obtained on exactly the same states but using the cost-to-goal estimates obtained during sampling for training the NN. Note that the results for \bfs with estimated costs to the goal differ from those with exact values in \cref{tab:small-sampling-hstar}. This happens because, during regression with \bfs, the cost-to-goal estimates are only exact on partial states; when turning them to complete states, the estimates can be larger than \hstar. Thus \hnnl{\bfs} with the estimates obtained during regression is less informed.

We can see that the relative order of the methods concerning the number of expanded states remains the same, although all methods expand more states. The increase in the number of expanded states is highest for \hnnl{\dfs}, which expands about seven times more states. In contrast, the other methods expand about twice as much, meaning that the estimates produced by \dfs during regression are inferior to those produced by the other methods. The mean \h-values confirm this: we can see that DFS significantly overestimates the true distances. Although \bfs has an estimation quality close to \hstar-value, its expanded states also degrade. These results suggest that sampling more states in localized regions of the state space~(\bfs closer to the goal and \dfs more distant) is insufficient to achieve good results during the search with \gbfs. Furthermore, \hnnl{\bfsrw} expands fewer states than \hnnl{\rw} and is the best in five of seven domains. Because \hnnl{\bfsrw} had a lower increase in expansions compared to \hnnl{\rw}, we focus on \bfsrw in the remaining experiments.

\begin{table}[tb]
    \caption[Comparison of sampling strategies on estimated \h-values.]{Comparison of sampling strategies \bfs, \dfs, \rw, and \bfsrw on estimated \h-values. Expanded states of \gbfs with learned heuristics and mean \h-values over the entire forward state space and the generated sample sets.}
    \label{tab:small-sampling-h}
    \addmargin
    \centering
    \input{tables/small-sampling-h}
\end{table}

\subsubsection{Maximum Regression Limit}
\label{sec:small-exps-rollout-limit}

In this experiment, we analyze the influence of the regression limit on the number of expanded states of sample generation strategy \bfsrw. We compare a fixed regression limit of~$L=200$ to setting the regression limit to the number of facts~\facts, or the number of facts divided by the mean number of effects~\meanfx. Values \facts and \meanfx for the selected tasks and the largest distance of any state from a goal state~\ssdiameter are shown in \cref{tab:small-rollout-limit}. Both \facts and \meanfx overestimate the largest distance \ssdiameter, except for \meanfx in three domains~(namely Blocks, Scanalyzer, and VisitAll). As discussed in \cref{sec:rollout-limit}, this is desirable since random walk rollouts do not follow the shortest paths.

\begin{table}[tb]
    \caption[State space information and expanded states with different regression limits.]{State space information and expanded states of \gbfs guided by \hnn trained on \bfsrw samples with different regression limits and no \h-value improvements. The value \ssdiameter is the distance of the state most distant from a goal state.}
    \label{tab:small-rollout-limit}
    \addmargin
    \centering
    \input{tables/small-rollout-limit}
\end{table}

The right-hand side of \cref{tab:small-rollout-limit} gives the number of expanded states for the three settings of $L$. We see that limits \facts and \meanfx perform better than the fixed limit~$200$, with \facts best on one, \meanfx on four, and $200$ on two tasks. Also, when a limit of~$200$ is best, the limit~\facts presents the closest results, but when \facts or \meanfx are best, a limit~$200$ can be much worse. Note that \facts is the best only in the domain where \meanfx underestimates \ssdiameter. To validate this, we set~$L=\ceil{c\bar F}$ for $c\in\{1.25,1.5,2,2.5,3,3.5\}$ in an additional experiment on domain Blocks. The number of expanded states decreases to~$c=3$ with a mean of $52.31$ expansions. Overall, the adaptive limits \facts and \meanfx are better estimates of the best regression limit.

\subsubsection{Randomly Generated Samples}
\label{sec:small-exps-random-samples}

In this experiment, we evaluate the effect of adding randomly generated samples to the sample set, as explained in \cref{sec:random-samples}. We generate sample sets~$S=\{(s_1,h_1),\ldots,(s_N,h_N)\}$ where $10\,\%, 20\,\%,\ldots,100\,\%$ are random samples~(to which the improvement strategies SAI and SUI are not applied) and the rest is sampled with \bfsrw and a regression limit~\meanfx. Random samples get an \h-value of~$H+1$ where $H=\max_{i\in[N]} h_i$ is the largest \h-value in samples~$S$, except when they are part of the samples, in which case they receive the corresponding estimate~(this happens in fewer than $1\,\%$ of the states). Note that when using $100\,\%$ of random samples, each has the cost-to-goal estimate equal to the regression limit~$L+1$ instead of~$H+1$, as we do not have samples in~$S$.

\begin{table}[tb]
    \caption[Expanded states from a varying percentage of randomly generated samples.]{Expanded states of \gbfs with a learned heuristic over samples generated by \bfsrw with regression limit~\meanfx, all cost-to-goal improvement strategies, and a varying percentage of randomly generated samples.}
    \label{tab:small-random-samples}
    \addmargin
    \centering
    \input{tables/small-random-samples}
\end{table}

\cref{tab:small-random-samples} shows the performance up to~$70\,\%$ random samples. We have omitted $80\,\%$ and $90\,\%$ since expansions are higher~(namely $56.10$ and $73.94$). The number of expansions is considerably reduced when using random samples, with $20\,\%$~random samples performing slightly better than other percentages. This also holds for individual domains, except Transport which expands on average a few states more, and N-Puzzle and Rovers, which expand a few states less.

To better understand the effect of random samples, we have performed three additional experiments with $20\,\%$ of random samples. The first focuses on cost-to-goal estimates. We keep the samples but replace $H+1$ with small values: a random \h-value from the sample set~$S$ or a random value drawn from $U[1,5]$. This leads to overall means of $295.65$ and $3832.14$ expanded states, respectively. The second experiment changes the distribution of the random samples: we force them to be part of the FSS. This leads to a mean of $36.90$ expanded states. Finally, the third experiment does not apply mutexes, leading to a mean of $36.11$ expanded states. From these additional experiments, it is clear that the most relevant factor is a high \h-value, and the distribution and quality of the states seem to matter less. Overall, the most probable explanation for the effect of random samples is that they help to increase the probability that the search is guided towards samples for which the network has learned good estimates, i.e.,~the samples obtained through regression.

\subsubsection{State Completion}
\label{sec:small-exps-state-completion}

Now we focus on how sampled partial states are converted to complete states. In this experiment, all the samples have perfect cost-to-goal estimates~\hstar. We compare three different state completion strategies for a partial state~$s$. All of them select a random state from the set of states represented by $s$ or a restriction of it: the set equals either to all states in~$s$~(no restrictions), only those states that satisfy mutexes, or only states from the forward state space~(perfect baseline).

\cref{tab:small-state-completion} presents the expanded states for these approaches. We can see that applying mutexes has a moderate effect and is very close to an ideal completion of the states. However, completing randomly also presents competitive results, except for N-Puzzle. Also, N-Puzzle with both restrictions should have similar results, as for this particular domain, all partial states completed respecting the mutexes are part of the FSS, but this is not the case due to noisy training (detailed in \cref{sec:noisy-training}). To confirm this, we ran $900$ experiments~, with $30$~sample seeds and $30$~network seeds, for N-Puzzle ``Mutex'' and ``FSS'', and we achieved similar mean expansions of $84.4$ and $88.66$, respectively.

\begin{table}[tb]
    \caption[Expanded states from different state completion approaches.]{Expanded states of \gbfs with \hnn trained with \bfsrw, \rlmeanfx, \hstar cost-to-goal estimates, and different state completion approaches.}
    \label{tab:small-state-completion}
    \addmargin
    \centering
    \input{tables/small-state-completion}
\end{table}

\subsection{Estimates of Cost-to-Goal}
\label{sec:small-exps-hvalue}

Our second set of experiments aims to analyze how techniques used to improve cost-to-goal estimates influence sample quality. To this end, we directly compare the difference of the improved estimates to \hstar-values. Then, to see how well the learned heuristics generalize, we evaluate them over the forward state spaces from \cref{tab:small-fss-size}.

\subsubsection{Quality of Estimates}
\label{sec:small-exps-hvalue-quality}

First, we compare the quality of the cost-to-goal estimates to \hstar with and without \h-value~improvement techniques and distinct regression limits, as shown in \cref{tab:small-difference-hstar}. This table shows the mean absolute difference between the sample estimates and \hstar, so smaller means indicate better approximations. Note that we are not evaluating an NN's output but the sample set's cost-to-goal estimates.

\begin{table}[tb]
    \caption[Mean difference of the cost-to-goal estimates to \hstar.]{Mean difference of the cost-to-goal estimates of samples of the sample set to \hstar.}
    \label{tab:small-difference-hstar}
    \addmargin
    \centering
    \input{tables/small-difference-hstar}
\end{table}

The improvement strategies SAI and SUI substantially reduce the estimates for all regression limiting methods. For \rldefault, \rlfacts, and \rlmeanfx, using only SAI reduces the estimates to $31.28$, $13.95$, and $4.93$, respectively, and using only SUI reduces the estimates to $11.1$, $5.48$, and $1.93$ respectively. Thus, SUI has the most effect on improving the cost-to-goal estimates compared to SAI.
Also, the adaptive regression limiting methods are superior to the fixed default~\rldefault, and \rlmeanfx has the best results. When comparing \rldefault to \rlmeanfx without \h-value improvements, the cost-to-goal estimate difference to \hstar decreases by about $6$ times on geometric mean. Blocks has the best performance, improving more than $25$~times. Finally, using both \h-value improvements and \rlmeanfx reduces the difference to \hstar from $33.45$ to only $1.60$.

\subsubsection{Evaluation Over the Forward State Spaces}
\label{sec:small-exps-hvalue-fss}

We now analyze the quality of our learned heuristics and the logic-based heuristics FF~\hff~\cite{hoffmann2001ff} and goal-count~\hgc over all states from the FSS of each task. \cref{tab:small-heuristics-difference-hstar} shows the results. Except for the baseline, the samples are generated with \bfsrw limited by \rldefault, \rlfacts, or \rlmeanfx and improved with SAI and SUI. The learned heuristic~\hnnrs is the same as \hnnl{\rlmeanfx}, but $20\,\%$ of the samples are randomly generated. We see that \hnnl{\rlmeanfx} reduces the difference of the predicted result to the real one by about $11$ times when compared to \hnnbase, and when compared to \hgc, it has the smallest difference in all domains except Scanalyzer. Also, \hnnl{\rlmeanfx} presents a similar mean difference to \hff. Due to the randomly generated samples in the sample set, \hnnrs doubles the difference compared to \hnnl{\rlmeanfx}. The only domain with a lower value is Blocks, driven by approximately two-thirds of the FSS states having an \hstar-value within a range of two or less the value assigned for random samples, thus improving the average.

\begin{table}[tb]
    \caption[Mean difference of heuristics to \hstar when evaluated over the FSS.]{Mean difference of \hff, \hgc and \hnn, to \hstar when evaluated over the FSS.}
    \label{tab:small-heuristics-difference-hstar}
    \addmargin
    \centering
    \input{tables/small-heuristics-difference-hstar}
\end{table}

Comparing \cref{tab:small-difference-hstar,tab:small-heuristics-difference-hstar}, we observe that relative order between \rldefault, \rlfacts, and \rlmeanfx is preserved. The mean difference of the samples' estimates to \hstar for \rlmeanfx is $1.60$ in \cref{tab:small-difference-hstar}, and when the corresponding~\hnnl{\rlmeanfx} is required to generalize over the entire FSS, the mean difference is $3.57$.

\subsection{Comparison to Logic-Based Heuristic Functions}
\label{sec:small-exps-hvalue-comparison}

We now compare the NN-based heuristics with logic-based heuristics. The number of expanded states of \gbfs guided by different heuristic functions is shown in \cref{tab:small-heuristics}. The NNs are trained with samples obtained with \bfsrw, all cost-to-goal improvement strategies, and regression depth limited by \rlmeanfx.  

\begin{table}[tb]
    \caption[Expanded states of different heuristic functions.]{Expanded states of \gbfs with different heuristic functions. The ``\hstar'' column is ideal and only used for comparison.}
    \addmargin
    \label{tab:small-heuristics}
    \centering
    \input{tables/small-heuristics}
\end{table}

\begin{table}[tb]
    \caption[Expanded states from different sample set sizes.]{Expanded states of \gbfs with \hnnl{\rlmeanfx} trained with a number of samples corresponding to some percentage of the number of states in the FSS of each task.}
    \label{tab:small-sample-pct}
    \addmargin
    \centering
    \input{tables/small-sample-pct}
\end{table}

First, we see that \hnnbase expands fewer states than \hgc in most domains except Grid, Scanalyzer, and VisitAll, but it is far worse than \hff except in Blocks and VisitAll, where the learned heuristic has particularly good results. We also see that \hnnl{\rlmeanfx} expands less than \hnnbase in five domains. \hff has better results than \hnnl{\rlmeanfx}. However, \hnnl{\rlmeanfx} surpasses \hff if $20\,\%$ of the samples are random, or if we increase the budget of \hnnl{\rlmeanfx} to $5\,\%$~(instead of $1\,\%$) of the number of states in the FSS as shown in \cref{tab:small-sample-pct}. This table also indicates that after increasing the budget to $50\,\%$ of the number of states in the FSS, the gains in quality of the learned heuristic are negligible. Additionally, by comparing \cref{tab:small-heuristics,tab:small-sample-pct}, we see that having fewer samples but \hstar estimates has better results than having more samples but no \hstar estimates, meaning that improving the cost-to-goal estimates is more important than having more samples.

We see that for the NN-based heuristics, the order of approaches in terms of \mbox{\h-\hstar}~difference~(\cref{tab:small-heuristics-difference-hstar}) and expanded states~(\cref{tab:small-heuristics}) remains consistent for \hnnbase and \hnnl{\rlmeanfx}, but not for \hnnrs, which has a higher mean difference than \hnnl{\rlmeanfx} but presents the least state expansions, even when compared to \hff. With these results, we conclude that a better generalization over the forward state space is good for the samples obtained during regression. In contrast, despite worsening the mean difference to the FSS, random samples are obtained after the regression procedure and can be helpful due to the reasons discussed in \cref{sec:small-exps-distribution}. This means that a smaller \mbox{\h-\hstar}~difference is not a definitive indication of good search quality.

\section{Large State Spaces}
\label{sec:large-experiments}

The main goal of the following experiments is to verify our findings from the previous sections on large state spaces, so we compare different configurations of the improved methods with logic-based heuristics and a baseline. We report results over $9$~seeds~($3$~network seeds and $3$~sample seeds), and using $50$~initial states from each of \citet{ferber2022neural} moderate tasks, which are the IPC tasks from $10$ selected domains that according to their results are solvable by \gbfs with \hff within $1$ to $900$~seconds. Each domain has the following number of tasks: Blocks,~$5$; Depot,~$6$; Grid,~$2$; N-Puzzle,~$8$; Pipes-NT,~$10$; Rovers,~$8$; Scanalyzer,~$6$; Storage,~$4$; Transport,~$8$; VisitAll,~$6$.

We generate samples within $1$~hour and set $1$~hour as the training time. Each of the $50$~initial states must be solved separately with \gbfs within $5$~minutes and $2$\,GB~RAM. Generally, more samples yield better results; however, because we do not know how much time will be spent on the \h-value improvement SUI phase as it is done after regression, we fix the number of samples at $N = 16\text{M} / |\mathcal{V}|$, which results in a mean of $500$\,MB RAM during sampling and $2$\,GB during SUI.

First, we reassess our previous results using the regression limits~\rlfacts and \rlmeanfx on large state spaces since our previous experiments~(Section \ref{sec:small-exps-distribution}) produced similar results. \cref{tab:large-rolloutlimit-mutex} shows the mean coverage and number of expanded states for the methods using \rlfacts or \rlmeanfx, with all \h-value improvements. In addition, we explore model-free variants~(denoted by \hnnnomutex) that use the state completion strategy without mutexes since we want to assess the performance of learning over samples generated with or without information from the task. Note that despite not using mutexes, the approach with \hnnnomutexl{\rlmeanfx} is still partially model-based as, during sampling, it infers the number of effects from the input task, while \hnnnomutexl{\rlfacts} is a model-free approach.

\begin{table}[tb]
    \caption[Results of the learned heuristics with and without mutexes.]{Mean coverages and expanded states of the learned heuristics with regression limits and their respective approaches not using mutexes~(\hnnnomutex). Expanded states consider only the initial states solved by all heuristics; Grid, N-Puzzle, and Storage had no common solved initial state. The geometric mean is used for the overall mean of expanded states.}
    \label{tab:large-rolloutlimit-mutex}
    \addmargin
    \centering
    \input{tables/large-rolloutlimit-mutex}
\end{table}

When comparing the learned heuristic~\hnnl{\rlmeanfx} over \hnnl{\rlfacts}, we see a mean coverage improvement of about $9\,\%$. All domains are improved or have very similar results, except VisitAll, where limiting the regression limit by \rlfacts is better -- this is also observed in the small state space experiments. Without mutexes, the coverage improvements from \rlmeanfx over \rlfacts are minor. However, the smaller number of expanded states in \hnnnomutexl{\rlmeanfx} indicates samples of higher quality, which achieves expansions close to when using mutexes. With or without mutexes, using \rlmeanfx has the highest positive effect in N-Puzzle, increasing its coverage by about four times. Also, not using mutexes improves Blocks, Depot, and Transport results while having a minimal effect on Pipes-NT, Rovers, Scanalyzer, and VisitAll. This suggests that model-free approaches show potential in these domains. Based on the results, we conclude that \rlmeanfx performs better than \rlfacts for large state spaces. Therefore, the following experiments will use \rlmeanfx.

Next, we compare the logic-based heuristics~\hff and \hgc, the baseline \hnnbase, and the best approach \hnnrs. The results are presented in \cref{tab:large-heuristics-baseline}. We see that \hff dominates in most domains, achieving twice the mean coverage of the baseline~\hnnbase. However, \hnnrs has only $12\,\%$ less mean coverage than \hff, improving \hnnbase by about $31\,\%$, with competitive coverage in most domains. Note that \hnnrs achieves better mean coverage than \hgc, with higher or equal coverage in $6$ out of $10$~domains. Also, in all domains except Transport, the best-learned heuristic expands fewer states on the same initial states when compared to \hff, indicating that the learned heuristic is more informed and that the inferior coverage is an effect of the slower expansion speed of the NN-based heuristics. Note, however, that the expanded states are biased towards easier tasks, as they refer to commonly solved initial states across the approaches. Furthermore, when limiting \hff by the same number of expansions as the learned heuristic, \hff achieves coverage of $81.20$, meaning that it still excels in most states. Because the dataset used contains only tasks that are solvable by \hff within $900$~seconds, the results are also biased towards better performance with a search guided by \hff. 

\begin{table}[tb]
    \caption[Results of the logic-based, baseline, and the best learned heuristics.]{Mean coverages and expanded states of the logic-based heuristics \hff and \hgc compared to the baseline learned heuristic \hnnbase and the best learned heuristic \hnnrs, obtained via training over samples with \bfsrw, \rlmeanfx, $20$\,\% of random samples, and all \h-value improvement strategies. Expanded states consider only the initial states solved by all heuristics; N-Puzzle and Storage had no common solved initial state. The geometric mean is used for the overall mean of expanded states.}
    \label{tab:large-heuristics-baseline}
    \addmargin
    \centering
    \input{tables/large-heuristics-baseline}
\end{table}

When comparing \cref{tab:large-rolloutlimit-mutex,tab:large-heuristics-baseline}, we notice that all NN-based heuristics have similarly poor results in Rovers, independent of configuration. Considering only the learned heuristics, when using $20\,\%$ of random samples~(\hnnrs) instead of $0\,\%$~(\hnnl{\rlmeanfx}), there are intermediate improvements of about $15\,\%$ in Storage, Transport, and VisitAll, and a significant improvement in Pipes-NT, from approximately $18\,\%$ to $80\,\%$~coverage.

\section{Comparison to Other Methods}
\label{sec:large-exps-comparison}

Although we do not aim to systematically compare other methods due to distinct machine configurations or libraries, we try to qualitatively compare our work with \citet{ferber2022neural} and \citet{otoole2022sampling}. All methods share the same dataset and NN configuration -- differences include batch size, patience value, NN initialization functions, and percentages of data split into training and validation sets.

Regarding our method, the sampling and training procedures can take a combined time of up to $2$~hours, and we use a search time limit of $5$~minutes. \citet{ferber2022neural} perform sampling and training for up to $28$~hours, with a search time limit of $10$~hours. \citet{otoole2022sampling} spend an unreported amount of time to generate $100$\,K~samples and an average of $23$~minutes in training, with a search time-limit of $6$~minutes. \citet{ferber2022neural} use a validation method that consists in retraining the network up to three times if the learned heuristic is not able to solve with \gbfs more than $80\,\%$ of generated validation states with a search time-limit of $30$ minutes. \citet{otoole2022sampling} train $10$~networks for each state space and select the one with the best-performing heuristic according to the same validation method as \citet{ferber2022neural}.

Considering only the best configurations, \citet{ferber2022neural} and \citet{otoole2022sampling} perform backward search-based sampling using random walks while we combine breadth-first search with random walk. \citet{ferber2022neural} obtain states through regression and try to solve them using \gbfs with the current heuristic to obtain the plans, which are used as training data. They use bootstrapping, which consists of improving the learned heuristics by training with samples of increasing difficulty, starting with a random walk limit of $5$ and doubling it~(up to $8$~times) whenever \gbfs finds a plan for at least $95\,\%$ of the states obtained through regression. \citet{otoole2022sampling} perform $5$~rollouts with a regression limit of $L=500$ and use the current depth as cost-to-goal estimates; they use the Tarski planning framework~\cite{tarski2018github} to perform the regression procedure as opposed to Fast~Downward and developed a cost-to-goal improvement strategy equivalent to SAI, and $50\,\%$ of the samples are randomly generated as described in \cref{sec:random-samples}. All methods have a Boolean representation for the samples and use mutexes (obtained from Fast~Downward) to complete unassigned state variables.

We now compare the coverage results between all methods over the same tasks. We perform an additional set of experiments to show in \cref{tab:large-literature-comparison} the coverage results of our best method using $0\,\%$, $20\,\%$ and $50\,\%$ of random samples, obtained from training over $100$\,K~samples -- same quantity as \citet{otoole2022sampling}. We also show results reported by \citet{ferber2022neural}~\hboot and \citet{otoole2022sampling}~\hnrsl, both trained without validation; we did not re-run their experiments. Note that although the dataset is the same, the results are not fully comparable due to different machine configurations and time dedicated to sampling, training, and testing. Considering coverages, we notice that our methods have results more similar to \hnrsl than to \hboot, and higher coverage, except for Grid, Rovers, Scanalyzer, and Storage. Generating samples only through regression~(i.e.,~without solving states) and training afterward is faster when compared to bootstrapping. A significant limitation of bootstrapping is the high cost of generating samples, as the states generated by the backward random walk must be solved with the currently learned heuristic to produce plans used as samples. Both \hnrsl and our methods suggest that sampling using regression with improvement strategies~(such as SAI, SUI, and random samples) gives competitive results in most domains.

\begin{table}[tb]
    \caption[Mean coverage results of our approach compared to those in the literature.]{Mean coverage results of \hboot~\cite{ferber2022neural} and \hnrsl~\cite{otoole2022sampling}, with results obtained from their respective papers, and our best learned heuristic trained with $100$\,K~samples, from which $0$\,\%, $20$\,\% and $50$\,\% are randomly generated.}
    \label{tab:large-literature-comparison}
    \addmargin
    \centering
    \input{tables/large-literature-comparison}
\end{table}

According to \citet{otoole2022sampling}, the proportion of random samples in the final sample set has the most positive effect on coverage -- approximately doubling it when going from $0\,\%$ to $50\,\%$ of random samples ($34.7$ vs.~$59.9$, from their supplementary material). As seen in \cref{tab:large-literature-comparison}, we also notice an improvement from using random samples, although smaller. This improvement is more prominent in \citet{otoole2022sampling}, most likely due to their small quantity of rollouts~(only five), negatively influencing sample diversity, which is compensated by adding random samples. Our experiments show that all domains either improve or have similar results, and the mean coverage improves by about $10\,\%$. Also, except for Pipes-NT, we saw no improvements above $5\,\%$ using $50\,\%$ of random samples compared to $20\,\%$. This means that despite having the highest coverage with $20\,\%$, using $50\,\%$ of random samples can maintain similar coverage while improving sampling time, as random samples are generated faster than in regression and are not computed in the \h-value improvement strategies.

\section{Limitations}
\label{sec:limitations}

This chapter discusses the limitations and problems of methods based on learning heuristics with NNs. Understanding and addressing these limitations can help advance the field and develop more robust and effective learned heuristic functions.

\subsection{Validation Loss}
\label{sec:validation-loss}

A significant limitation is the unreliability of the validation loss as an indicator of performance during the training. We observe that two NNs trained with the same configuration\mri{Means? What configuration?}\rvi{I see no reason why this shouldn't happen in *any* configuration; if so, then we have the solution to this limitation.} have different validation losses, where the network with a larger loss may actually outperform the one with a smaller loss in terms of coverage or number of states expanded during the search.\mri{By how much? Give data.}\rvi{The data is in the next paragraph. I have merged both paragraphs together to make it easier for the reader to see this.} For instance, when training with \hstar-values over states generated with \bfsrw, with one sample seed and $100$ network seeds for each of the small state spaces, $36\,\%$ of the models with the least amount of expanded states had a higher validation loss\mri{(I didn't understand the first word)! Data!}\rvi{I don't think it aggregates information to write "a validation loss X bigger on average than other models". There's not even anything to compare it with; to that, we should also add the final validation loss, the difference in the number of expanded states, etc. I think the idea here is not to go into depth on this (also, the scope of the dissertation is not NN), it's just to let the reader know that it's a noted limitation; if necessary, I can move to the future work section.} than other models with more expanded states; if the states do not have \hstar-values, the percentage increases to $49\,\%$. Regarding coverage, in the large state space experiments with the heuristic \hnnrs and $9$ runs ($3$ sample seeds and $3$ network seeds) for each state space, $15\,\%$ of the models with the highest coverage had a higher validation loss than the other models with lower coverage. This happens because the sampling procedure is imperfect -- an NN learns a limited portion of the state space, which varies per sample seed, so even if the validation loss is small, it provides no guarantees of search quality.

This phenomenon challenges model validation methods and clarifies that a lower validation loss does not necessarily translate to a better heuristic function in the search. Consequently, relying solely on validation loss to measure model quality can lead to misleading conclusions and suboptimal decisions about when to stop training or which NN is most effective for the search. Using multiple seeds can work around the problem. However, alternative evaluation metrics and techniques must be explored to more accurately assess the performance and effectiveness of learned heuristic models before the search.

\subsection{Noisy Training}
\label{sec:noisy-training}

Another limitation to consider is the influence of seed initialization on the training. As noted by other methods that use some form of model validation~\cite{ferber2020neural, shen2020learning, ferber2022neural, otoole2022sampling}, training can be noisy, with multiple runs leading to very different results. A consequence of this is observed in our experiments comparing expanded states, where even a single state that ends up expanded due to an inaccurate heuristic can lead to numerous extra expansions. In contrast, other states can lead the search to a more direct path to the goal. This problem is common for all approaches that use \gbfs, but this is further aggravated in NNs that learn from approximated values.

\begin{table}[tb]
    \caption[Expanded states with standard deviations in small state space experiments.]{Expanded states with \gbfs and their standard deviations in small state space experiments using the baseline \hnnbase and the best heuristic \hnnrs.}
    \label{tab:small-stdev}
    \addmargin
    \centering
    \input{tables/small-stdev}
\end{table}

An example is shown in \cref{tab:small-stdev} comparing the mean number of expanded states of the baseline\mri{Expand}\rvi{I didn't understand} and \hnnrs. We see that standard deviation varies per domain, i.e.,~some domains are noisier than others, and training with better-quality samples typically helps. Regarding coverage in large state spaces, the standard deviation was smaller, as it does not vary at the same rate as expanded states. For example, \hnnrs has a standard deviation of $16\,\%$\mri{(I didn't understand the first word)? Base?}\rvi{I didn't understand} in Grid coverage, while the mean standard deviation considering all the other domains is less than $3\,\%$.

\subsection{State Representation}
\label{sec:limitation-representation}

This work and others~\cite{ferber2020neural, ferber2022neural, otoole2022sampling} use the same STRIPS representation, and the NN receives as input a vector of Boolean values representing the set of facts of a complete state, where each input neuron corresponds to a fact. However, this is inefficient when training over large planning tasks that can have an input size in the order of thousands.\mri{is it? Why?}\rvi{We saw that in the NIPS paper: Transport in large state spaces (thousands of input values) took 9 minutes each epoch = inefficient training}

Furthermore, sampling methods via regression generate partial states, representing a set of complete states. With the assignment of undefined variables, part of the sampling information is lost. \citet{yu2020learning} use the same STRIPS representation but does not complete the undefined variables, assigning false to all facts corresponding to an undefined variable. Considering undefined values as false is a way around the impossibility of representing undefined values in the Boolean input of the NN\mri{My god: that's not a fundamental philosophical problem; you're saying that a bit does not have three states. Well that's so by choice.}\rvi{I didn't understand}; however, it trains the network with a set of states that will never be encountered during the search\mri{How do you know? That is probably often false.}\rvi{If false is set for all atoms of an undefined variable, it is impossible for this state to be equal a complete state; and the search only sees complete states. This is only false if the state is already a complete state (but this sentence refers to completing partial states.)}.

LHFCP~\cite{geissmann2015learning} uses a multivalued \sas vector representation of the state, which aligns with the internal state representation in Fast~Downward. This choice allows the representation of undefined values and increases the speed of the NN by reducing the dimensionality of the input layer. However, since multiple facts share the same neuron weight, capturing patterns and relationships between facts can be impaired.\rvi{Originally: the main drawback lies in its inability to capture fine-grained distinctions among facts, as each input neuron corresponds to a set of facts}\mri{What are you talking about? Give an example of a "fine-grained distinction among states" as oppose to a coarse-grained one. Define the grain size, and tell us how we can recognize if a distinction is fine or coarse-grained.}\rvi{Why is it recommended to delve into topics completely outside our research rather than just removing the line or writing more simply?} This limitation could affect the accuracy of the learned heuristic function.
