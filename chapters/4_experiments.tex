\chapter{Experiments}
\label{sec:experiments}

In this section, we present two sets of experiments. In the first set, we analyze the behavior of sampling methods on planning tasks for which we can enumerate the complete state space with associated optimal cost-to-goal estimates \hstar. We study how different sampling methods influence the quality of the learned heuristic concerning the number of state expansions by a search algorithm. In the second set, we evaluate how our findings generalize to a practical setting with large planning tasks. Our methods are then compared to logic-based heuristics.

\section{Settings}
\label{sec:exp_settings}

We use a residual neural network~\cite{He.etal/2016} to learn a heuristic for a state space. The network's input is a Boolean representation of the states, where a fact is set to~$1$ if it is true in the state and~$0$ otherwise, as explained in Section~\ref{sec:background_learningheuristics}, and its output is a single neuron with the predicted $h$-value. The network has two hidden layers followed by a residual block with two hidden layers. Each hidden layer has~$250$ neurons that use ReLU activation and are initialized as proposed by He et al.~(\citeyear{He.etal/2015}). The training uses the Adam optimizer~\cite{Kingma.Ba/2015}, a learning rate of $10^{-4}$, an early-stop patience of~$100$, and a MSE loss function. Due to better results in preliminary experiments, we use batch sizes of $64$ for small and $512$ for large state spaces, and $90\,\%$ of the sampled data as the training set, with the remaining $10\,\%$ as the validation set. In the experiments, different learned heuristics are denoted as $\hat h_A$, where $A$ indicates different algorithmic choices.

We select the domains and tasks from Ferber et al.~(\citeyear{Ferber.etal/2022}), namely: Blocks, Depot, Grid, N-Puzzle, Pipes-NT, Rovers, Scanalyzer, Storage, Transport, and VisitAll. All domains have unit costs except for Scanalyzer and Transport, for which we consider the variant with unit costs. All methods are implemented on the Neural~Fast~Downward planning system with PyTorch~1.9.0~\cite{Ferber.etal/2020a,Paszke/2019}. Our source code, planning tasks, and experiments are available\footnote{Available at \url{https://github.com/bettker/NeuralFastDownward}}. All experiments were run on a PC with an AMD Ryzen~9 3900X processor, using a single core with $4$~GB RAM per process. We solve all tasks with \gbfs guided by the learned heuristic~\hnn with the lowest generation order as a tie-breaking strategy.

We note that an NN may fail to train if, after initialization, it outputs zero for all training samples (this is called ``born dead'' in Lu et al.~(\citeyear{Lu.etal/2020})). We noticed this occurs with a non-negligible frequency in experiments on smaller state spaces with fewer samples. Thus, we test for this condition, and if, after initialization, the network outputs zero for all samples in the training set, we reinitialize with a different seed until the network outputs a non-zero value for some sample.

We compare the experiments to a baseline \hnnbase with a similar configuration to previous approaches from the literature presented in Section~\ref{sec:background_relatedwork}. For the baseline, the NN is trained by a sampling method that uses random walks with a regression limit of~$200$ backward steps. In addition, mutexes are used during regression and for sample completion, but resetting the $h$-value to $0$ in goal states and the improvement strategies SAI and SUI are turned off. In preliminary experiments sampling with the FSM algorithm, we use $p_{\bfsrw}\in\{0.01,0.05,0.1,0.2,\ldots,0.9\}$. $p_{\bfsrw}=0.1$ achieved the best average performance. Thus, we fix $p_{\bfsrw}$ to this value in the experiments.

\section{Small State Spaces}
\label{sec:experiment1}

In this section, we study the behavior of different sampling methods on small state spaces. For each domain, we select the task with the largest size state space that can be enumerated completely to obtain \hstar-values. We only select tasks with state spaces with $30$\,K states or more, and fewer than $1$\,M states. Table~\ref{tab:small-instances} shows the tasks and their state space sizes. For domains Grid, Rovers, Scanalyzer, and Transport, the task had fewer than $30$\,K states, and VisitAll more than $1$\,M, so we manually modified these tasks. We could not find a task within our limits for Depot, Pipes-NT and Storage, so they were excluded from our experiments.
We generate the initial states for the small state spaces by performing a random walk of length~$200$ from the original initial state of a task. Rovers, Scanalyzer and VisitAll had duplicated initial states or that satisfied the goal state. Thus, for these domains, we generate the initial states with random walks of length $25$, $50$, and $8$, respectively.

\input{tables/small-instances}

In small state space experiments, the coverage for all methods is $100\,\%$. Therefore we use the number of expanded states to evaluate the quality of the heuristic function. In these experiments, we report means over experiments with five different network seeds, five different sample seeds, and $50$ initial states. The training time has been limited to $30$ minutes. If not stated otherwise, methods \bfs, \dfs, \rw, and \bfsrw use mutexes, the improvement strategies SAI and SUI, and the number of samples is equal to $1\%$ of the state space size. After initialization, the percentage of networks that output $0$ for all samples was at most $40\,\%$ (in Blocks), and all networks successfully passed the initialization test after the second initialization. Under these conditions, less than $1.5\,\%$ of the NNs did not converge within the time limit.

\subsection{Distribution of States}
\label{sec:experiment1-subset}

Our first set of experiments aims to analyze the influence of the distribution of sampled states in the state space on the quality of the learned heuristics. For this, we compare different sample generation algorithms, regression limits, random sample percentages, and state completion. If not stated otherwise, for the parameters that are not varied, we use the baseline setting defined above (regression limit $L=200$,  mutexes, no cost-to-goal improvement methods).

\subsubsection{Sample Generation Algorithms}

In this experiment, we compare four sample generation algorithms: \bfs, \dfs, \rw, and \bfsrw. To control the effect of the cost-to-goal estimates on the quality of the learned heuristic, we replace sample estimates with optimal values $h^{*}$ before training. Table~\ref{tab:small-h-optimal} shows the number of expanded states of a \gbfs guided by the learned heuristics and the mean $h^{*}$-values over the sampled states. We see that heuristic \hnnbfs leads to more expanded states than \hnndfs, which in turn expands about $50\,\%$ more states than \hnnrw and \hnnbfsrw, which perform similarly. Using \hnnbfs is significantly worse and leads to the highest or close to the highest number of expansions in all domains. Heuristic \hnndfs has a high number of expansions in Blocks, N-Puzzle, and Transport. Looking at the mean $h^{*}$-values, we see that samples generated by \bfs have the lowest, and those by \dfs the highest mean estimates in all domains. Although the distribution of \dfs is closest to that of the whole state space (inferred from the mean $h^{*}$-values), the resulting heuristic expands more states than \rw and \bfsrw, which generate states closer to the goal. Therefore, multiple random walk rollouts seem better than a single rollout with \bfs or \dfs due to increased sample diversity in different portions of the state space, covering states more likely to be visited during the search.

\input{tables/small-h-optimal}

We now compare these results to results shown in Table~\ref{tab:small-h-estimate}, obtained on exactly the same states but using the cost-to-goal estimates obtained during sampling for training the NN. Note that the results for \bfs with estimated costs to the goal differ from those with exact values in Table~\ref{tab:small-h-optimal}. This happens because, during regression with \bfs, the cost-to-goal estimates are only exact on partial states; when turning them to complete states, the estimates can be larger than \hstar. Thus \hnnbfs with the estimates obtained during regression is less informed.

We can see that the relative order of the methods concerning the number of expanded states remains the same, although all methods expand more states. The increase in the number of expanded states is highest for \hnndfs, which expands about seven times more states. In contrast, the other methods expand about twice, meaning that the estimates produced by \dfs during regression are inferior to those produced by the other methods. The mean $h$-values confirm this: we can see that DFS significantly overestimates the true distances. Although \bfs has an estimation quality close to \hstar-value, its expanded states also degrade. These results suggest that sampling more states in localized regions of the state space (\bfs closer to the goal and \dfs more distant) is insufficient to achieve good results during the search with \gbfs.
Furthermore, \hnnbfsrw expands fewer states than \hnnrw and is the best in five of seven domains. Because \hnnbfsrw had a lower increase in expansions compared to \hnnrw, we focus on \bfsrw in the remaining experiments.

\input{tables/small-h-estimate}

\subsubsection{Maximum Regression Limit}

In this experiment, we analyze the influence of the regression limit on the number of expanded states of sample generation strategy \bfsrw. We compare a fixed regression limit of $L=200$ to setting the regression limit to the number of facts $F$, or the number of facts divided by the mean number of effects $\bar F$. Values $F$ and $\bar F$ for the selected tasks, and additionally the largest distance of any state from the goal state~\distfarthest, are shown in Table~\ref{tab:small-strategies-limits}. Both $F$ and $\bar F$ overestimate the largest distance \distfarthest, except for $\bar F$ in three domains (namely Blocks, Scanalyzer, and VisitAll). As discussed in Section~\ref{sec:rollout-depth-limit} this is desirable since random walk rollouts do not follow the shortest paths.

\input{tables/small-strategies-limits}

The right-hand side of Table~\ref{tab:small-strategies-limits} gives the number of expanded states for the three settings of $L$. We see that limits $F$ and $\bar F$ perform better than the fixed limit $200$, with $F$ best on one, $\bar F$ on four, and $200$ on two tasks. Also, when a limit of $200$ is best, the limit $F$ presents the closest results, but when $F$ or $\bar F$ are best, a limit $200$ can be much worse. Note that $F$ is the best only in the domain where $\bar F$ underestimates \distfarthest. To validate this, we set $L=\ceil{c\bar F}$ for $c\in\{1.25,1.5,2,2.5,3,3.5\}$ in an additional experiment on domain Blocks. The number of expanded states decreases to $c=3$ with a mean of $52.31$ expansions. Overall, the adaptive limits $F$ and $\bar F$, although imperfect, are better estimates of the best regression limit.

\subsubsection{Randomly Generated Samples}

In this experiment, we evaluate the effect of adding randomly generated samples to the sample set, as explained in Section~\ref{sec:random-samples-theory}. We generate sample sets $S=\{(s_1,h_1),\ldots,(s_N,h_N)\}$ where $10\,\%, 20\,\%,\ldots,100\,\%$ are random samples (to which the improvement strategies SAI and SUI are not applied) and the rest is sampled with \bfsrw and a regression limit $\bar F$. Random samples get an $h$-value of $H+1$ where $H=\max_{i\in[N]} h_i$ is the largest $h$-value in samples $S$, except when they are part of the samples, in which case they receive the corresponding estimate (this happens in fewer than $1\,\%$ of the states). Note that when using $100\,\%$ of random samples, each has the cost-to-goal estimate equal to the regression limit $L+1$ instead of $H+1$, as we do not have samples in $S$.

\input{tables/small-contrasting}

Table~\ref{tab:small-contrasting} shows the performance up to $70\,\%$ random samples. We have omitted the larger values since expansions are higher (namely $56.10$, $73.94$, and $11397.79$). The number of expansions is considerably reduced when using random samples, with $20\,\%$ random samples performing slightly better than other percentages. This also holds for individual domains, except Transport which expands on average a few states more, and N-Puzzle and Rovers, which expand a few states less.

To better understand the effect of random samples, we have performed three additional experiments with $20\,\%$ of random samples. The first focuses on cost-to-goal estimates. We keep the samples but replace $H+1$ with small values: a random $h$-value from the sample set $S$ or a random value drawn from $U[1,5]$. This leads to overall means of $295.65$ and $3832.14$ expanded states, respectively. The second experiment changes the distribution of the random samples: we force them to be part of the FSS. This leads to a mean of $36.90$ expanded states. Finally, the third experiment does not apply mutexes, leading to a mean of $36.11$ expanded states. From these additional experiments, it is clear that the most relevant factor is a high $h$-value, and the distribution and quality of the states seem to matter less. Overall, the most probable explanation for the effect of random samples seems to be that they help to increase the probability that the search is guided towards samples for which the network has learned good estimates, i.e., the samples obtained through regression.

\subsubsection{State Completion}

Now we focus on how sampled partial states are converted to complete states. In this experiment, all the samples have optimal cost-to-goal estimates \hstar. We compare three different sample completion strategies for a partial state $s$. All of them select a random state from the set of states represented by $s$ or a restriction of it: the set equals either to all states in $s$ (no restrictions), only those states that satisfy mutexes, or only states from the forward state space (perfect baseline).

Table~\ref{tab:small-estimates-mutex} presents the expanded states for these approaches. We can see that applying mutexes has a moderate effect and is very close to an ideal completion of the states. However, completing randomly also presents competitive results, except for N-Puzzle. Also, N-Puzzle with both restrictions should have similar results -- as for this particular domain, all partial states completed respecting the mutexes are part of the FSS~--, but this is not the case, as N-Puzzle is a particularly noisy domain with a high standard deviation of state expansions, aggravated by \gbfs. To verify this, we ran $900$ experiments ($30$ sample seeds and $30$ network seeds) for N-Puzzle ``Mutex'' and ``FSS'', and we achieved similar mean expansions of $84.4$ and $88.66$, respectively.

\input{tables/small-estimates-mutex}

\subsection{Estimates of Cost-to-Goal}

Our second set of experiments aims to analyze how techniques used to improve cost-to-goal estimates influence sample quality. To this end, we directly compare the difference of the improved estimates to \hstar-values. Then, to see how well the learned heuristics generalize, we evaluate them over the forward state spaces from Table~\ref{tab:small-instances}.

\subsubsection{Quality of Estimates}

First, we compare the quality of the cost-to-goal estimates to \hstar with and without $h$-value improvement techniques and distinct regression limits, as shown in Table~\ref{tab:small-samples-ssp}. This table shows the mean absolute difference between the sample estimates and \hstar, so smaller means indicate better approximations. Note that we are not evaluating the output of an NN but the estimates part of the sample set.

\input{tables/small-samples-ssp}

The improvement strategies SAI and SUI substantially reduce the estimates for all regression limiting methods. For \default, \facts and \meanfx, using only SAI reduces the estimates to $31.28$, $13.95$, and $4.93$, respectively, and using only SUI reduces the estimates to $11.1$, $5.48$, and $1.93$ respectively. Thus, SUI has the most effect on improving the cost-to-goal estimates compared to SAI.
Also, the adaptive regression limiting methods are superior to the fixed default \default, and \meanfx has the best results. When comparing \default to \meanfx without $h$-value improvements, the cost-to-goal estimate difference to \hstar decreases by about $6$ times on geometric mean. Blocks has the best performance, improving more than $25$ times. Finally, using both $h$-value improvements and \meanfx reduces the difference to \hstar from $33.45$ to only $1.60$.

\subsubsection{Evaluation Over the Forward State Spaces}

We now analyze the quality of our learned heuristics and the logic-based heuristics fast-forward \hff~\cite{Hoffmann.Nebel/2001} and goal-count \hgc over all states from the FSS of each task. Table~\ref{tab:small-strategies-sp-error} shows the results. Except for \hnnbase (baseline), the samples are generated with \bfsrw limited by \default, \facts or \meanfx, and improved with SAI and SUI. The learned heuristic \hnnrs is the same as \hnnbfsrwl{\meanfx}, but $20\,\%$ of the samples are randomly generated. We see that \hnnbfsrwl{\meanfx} reduces the difference of the predicted result to the real one by about $11$ times when compared to \hnnbase, and when compared to \hgc, it has the smallest difference in all but two domains. Also, \hnnbfsrwl{\meanfx} presents a similar mean difference to \hff. Due to the randomly generated samples in the sample set, \hnnrs doubles the difference compared to \hnnbfsrwl{\meanfx}, with a smaller value only in Blocks.

\input{tables/small-strategies-sp-error}

Comparing Tables~\ref{tab:small-samples-ssp} and \ref{tab:small-strategies-sp-error}, we observe that relative order between \default, \facts, and \meanfx is preserved. The mean difference of the samples' estimates to \hstar for \meanfx is $1.60$ in Table~\ref{tab:small-samples-ssp}, and when the corresponding~\hnnbfsrwl{\meanfx} is required to generalize over the entire FSS, the mean difference is $3.57$.

\subsection{Comparison to Logic-Based Heuristic Functions}

We now compare the NN-based \hnn with logic-based heuristics. The number of expanded states of \gbfs guided by different heuristic functions is shown in Table~\ref{tab:small-samples-heuristic}. The NNs are trained with samples obtained with \bfsrw, all cost-to-goal improvement strategies and regression depth limited by \meanfx.  

\input{tables/small-samples}

\input{tables/small-samples-pct}

First, we see that \hnnbase expands fewer states than \hgc in most domains except Grid, Scanalyzer, and VisitAll, but it is far worse than \hff except in Blocks and VisitAll, where the learned heuristic has particularly good results. We also see that \hnnbfsrwl{\meanfx} expands less than \hnnbase in five domains. Note that \hff has better results than \hnnbfsrwl{\meanfx}. However, \hnnbfsrwl{\meanfx} surpasses \hff if $20\,\%$ of the samples are random, or if we increase the budget of \hnnbfsrwl{\meanfx} to $5\,\%$ (instead of $1\,\%$) of the number of states in the FSS as shown in Table~\ref{tab:small-samples-pct}. This table also indicates that after increasing the budget to $50\,\%$ of the number of states in the FSS, the gains in quality of the learned heuristic are negligible. Additionally, by comparing Tables~\ref{tab:small-samples-heuristic} and \ref{tab:small-samples-pct}, we see that having fewer samples but \hstar estimates has better results than having more samples but no \hstar estimates, meaning that improving the cost-to-goal estimates is more important than having more samples.

Comparing Tables~\ref{tab:small-strategies-sp-error} and~\ref{tab:small-samples-heuristic}, we see that for the NN-based heuristics, the number of expanded states is linked to the $h$--\hstar difference. An exception is \hnnrs, which has a higher mean difference than \hnnbfsrwl{\meanfx} but presents the least state expansions, even when compared to \hff. With these results, we conclude that a better generalization over the forward state space is good for the samples obtained during regression. In contrast, despite worsening the mean difference to the FSS, random samples are obtained after the regression procedure and can be helpful due to the reasons discussed in Section~\ref{sec:experiment1-subset}. This means that a smaller $h$-value difference to the FSS is not a definitive indication of good search quality.

\section{Large State Spaces}
\label{sec:experiment2}

The main goal of the following experiments is to verify our findings from the previous sections on large state spaces, so we compare different configurations of the improved methods with logic-based heuristics and a baseline. We report results over $9$ seeds ($3$ network seeds and $3$ sample seeds), and using $50$ initial states from each of \citeyear{Ferber.etal/2022} moderate tasks, which are the International Planning Competition tasks from $10$ selected domains that according to their results are solvable by \gbfs with \hff within $1$ to $900$ seconds. Each domain has the following number of tasks: Blocks, $5$; Depot, $6$; Grid, $2$; N-Puzzle, $8$; Pipes-NT, $10$; Rovers, $8$; Scanalyzer, $6$; Storage, $4$; Transport, $8$; VisitAll, $6$. Details are available in our supplementary material.

The time to generate the samples lasts up to $1$ hour. For training, we set $1$ hour, and each of the $50$ initial states must be solved separately with \gbfs within $5$ minutes and $2$\,GB RAM. Generally, more samples yield better results; however, because we do not know how much time will be spent on the $h$-value improvement SUI phase as it is done after regression, we fix the number of samples at $N = \frac{16\text{M}}{|\mathcal{V}|}$, which results in a mean of $500$\,MB RAM during sampling and $2$\,GB during SUI.

First, we reassess our previous results using the regression limits \facts and \meanfx on large state spaces since our previous experiments (Section \ref{sec:experiment1-subset}) produced similar results. Table~\ref{tab:large-instances-moderate-nomutex} shows the mean coverage and number of expanded states for our methods using \facts or \meanfx, with all $h$-value improvements, along with a model-based and a mutex-free version (denoted by \hnnnomutex). The distinction between model-based and model-free is made because we want to assess the performance of learning over samples that were generated using information from the task or not. Note that despite not using mutexes, the approach with \hnnnomutexl{\meanfx} is still partially model-based as, during sampling, it infers the number of effects from the input task, while \hnnnomutexl{\facts} is a model-free approach.

\input{tables/large-instances-moderate-nomutex}

When comparing the learned heuristic \hnnbfsrwl{\meanfx} over \hnnbfsrwl{\facts}, we see a mean coverage improvement of about $9\,\%$. All domains are improved or have very similar results, except VisitAll, where limiting the regression limit by \facts is better -- this is also observed in the small state space experiments. Without mutexes, the coverage improvements from \hnnnomutexl{\meanfx} over \hnnnomutexl{\facts} are minor. However, the smaller number of expanded states indicates samples of higher quality, which achieves expansions close to when using mutexes. With or without mutexes, using \meanfx has the highest positive impact in N-Puzzle, increasing its coverage by about four times. Also, we see that not using mutexes improves the results in Blocks, Depot, and Transport while having a minimal effect in Pipes-NT, Rovers, Scanalyzer, and VisitAll, meaning that model-free approaches are promising. These results conclude that \meanfx performs better than \facts for large state spaces. Therefore, the following experiments will use \meanfx.

Next, we compare the traditional logic-based heuristics \hff and \hgc, the baseline \hnnbase, and the best approach \hnnrs to see where NN-based heuristics stand compared to traditional logic-based heuristics. The results are presented in Table~\ref{tab:large-instances-moderate-cmp}.
We see that \hff dominates in most domains, achieving twice the mean coverage of the baseline \hnnbase. However, \hnnrs has only $12\,\%$ less mean coverage than \hff, improving \hnnbase by about $31\,\%$, with competitive coverage in most domains. Note that \hnnrs achieves better mean coverage than \hgc, with higher or equal coverage in $6$ out of $10$ domains. Also, in all domains except Transport, the best-learned heuristic expands fewer states on the same initial states when compared to \hff, indicating that the learned heuristic is more informed and that the inferior coverage is an effect of the slower expansion speed of the NN-based heuristics. Note, however, that the intersection of solved states is biased towards states that are easier to solve. Furthermore, when limiting \hff by the same number of expansions as the learned heuristic, \hff achieves coverage of $81.20$, meaning that it still excels in most states. Because the dataset used contains only tasks that are solvable by \hff within $900$ seconds, the results are also biased towards better performance with a search guided by \hff. 

\input{tables/large-instances-moderate-cmp}

When comparing Tables~\ref{tab:large-instances-moderate-nomutex} and \ref{tab:large-instances-moderate-cmp}, we notice that all NN-based heuristics have similarly poor results in Rovers, independent of configuration. Considering only the learned heuristics, when using $20\,\%$ of random samples (\hnnrs) instead of $0\,\%$ (\hnnbfsrwl{\meanfx}), there are intermediate improvements of about $15\,\%$ in Storage, Transport and VisitAll, and a significant improvement in Pipes-NT, from approximately $18\,\%$ to $80\,\%$ coverage.

\section{Comparison to Other Methods}
\label{sec:comparison}

Although we do not aim to make a systematic comparison to other methods due to distinct machine configurations, libraries, or missing source code, we try to qualitatively compare our work with Ferber et al.~(\citeyear{Ferber.etal/2022}) and O'Toole et al.~(\citeyear{OToole/2022}). All methods share the same dataset and NN configuration -- differences include batch size, patience value, NN initialization functions, and percentages of data split into training and validation sets.

Regarding our method, the sampling and training procedures can take a combined time of up to $2$ hours, and we use a search time limit of $5$ minutes. Ferber et al.~(\citeyear{Ferber.etal/2022}) perform sampling and training for up to $28$ hours, with a search time limit of $10$ hours. O'Toole et al.~(\citeyear{OToole/2022}) spend an unreported amount of time to generate $100$\,K samples and an average of $23$ minutes in training, with a search time-limit of $6$ minutes. In the case of Ferber et al.~(\citeyear{Ferber.etal/2022}), they use a validation method that consists in retraining the network up to three times if the learned heuristic is not able to solve with \gbfs more than $80\,\%$ of generated validation states with a search time-limit of $30$ minutes. Meanwhile, O'Toole et al.~(\citeyear{OToole/2022}) train $10$ networks for each state space and select the one with the best-performing heuristic according to the same validation method as Ferber et al.~(\citeyear{Ferber.etal/2022}). We do not use any form of validation since we report the mean of multiple seeds.

Considering only the best configurations, Ferber et al.~(\citeyear{Ferber.etal/2022}) and O'Toole et al.~(\citeyear{OToole/2022}) have the sampling procedure based on backward searches from the goal with random walk, while we use a combination of breadth-first search with random walk. Ferber et al.~(\citeyear{Ferber.etal/2022}) obtain states through regression and try to solve them using \gbfs with the current heuristic to obtain the plans, which are used as training data. They use bootstrapping, which consists of improving the learned heuristics by training with samples of increasing difficulty, starting with a random walk limit of $5$ and doubling it (up to $8$ times) whenever \gbfs finds a plan for at least $95\,\%$ of the states obtained through regression. O'Toole et al.~(\citeyear{OToole/2022}) perform $5$ rollouts with a regression limit of $L=500$ and use the current depth as cost-to-goal estimates; they use the Tarski planning framework to perform the regression procedure as opposed to Fast~Downward and developed a cost-to-goal improvement strategy equivalent to SAI, and $50\,\%$ of the samples are randomly generated as described in Section~\ref{sec:random-samples-theory}. All methods have a boolean representation for the samples and use mutexes (obtained from Fast~Downward) to complete unassigned state variables.

We now compare the coverage results between all methods over the same tasks. We perform an additional set of experiments to show in Table~\ref{tab:large-instances-moderate-100k} the coverage results of our best method using $0\,\%$, $20\,\%$ and $50\,\%$ of random samples, obtained from training over $100$\,K samples -- same quantity as O'Toole et al.~(\citeyear{OToole/2022}). We also show results extracted from Ferber et al.~(\citeyear{Ferber.etal/2022}) \hboot and O'Toole et al.~(\citeyear{OToole/2022}) \hnrsl, both trained without validation; we did not re-run their experiments. Note that although the dataset is the same, the results are not fully comparable due to different machine configurations and time dedicated to sampling, training, and testing. Considering coverages, we notice that our methods have results more similar to \hnrsl than to \hboot, and except for Grid, Rovers, Scanalyzer, and Storage, a higher coverage. Generating samples only through regression (i.e., without solving states) and training afterward is faster when compared to bootstrapping: a significant limitation of it is the high cost of generating samples, as the states generated by the backward random walk must be solved with the currently learned heuristic to produce plans used as samples. As it stands, both \hnrsl and our methods suggest that sampling using regression with improvement strategies (such as SAI, SUI, and random samples) gives competitive results in most domains.

\input{tables/large-instances-moderate-100k}

According to O'Toole et al.~(\citeyear{OToole/2022}), the proportion of random samples in the final sample set has the most positive effect on coverage -- approximately doubling it when going from $0\,\%$ to $50\,\%$ of random samples ($34.7$ vs.~$59.9$, from their supplementary material). As seen in Table~\ref{tab:large-instances-moderate-100k}, we also notice an improvement from using random samples, although not as substantial. This improvement is more prominent in O'Toole et al.~(\citeyear{OToole/2022}), most likely due to their small quantity of rollouts (only five), negatively influencing sample diversity, which is compensated by adding random samples. Our experiments show that all domains either improve or have similar results, and the mean coverage improves by about $10\,\%$. Also, except for Pipes-NT, we saw no improvements above $5\,\%$ using $50\,\%$ of random samples compared to $20\,\%$. This means that despite having the highest coverage with $20\,\%$, using $50\,\%$ of random samples can maintain similar coverage while improving sampling time, as random samples are generated faster than in regression and are not computed in the $h$-value improvement strategies.
