\chapter{Experiments}
\label{sec:experiments}

In this section, we present two sets of experiments. In the first set, we analyze the behavior of sampling methods on planning tasks for which we can enumerate the complete state space with associated optimal cost-to-goal estimates \hstar. We study how different sampling methods influence the quality of the learned heuristic with regard to the number of state expansions by a search algorithm. In the second set, we evaluate how our findings generalize to a practical setting with large planning tasks. Our methods are then compared to logic-based heuristics.

\section{Settings}
\label{sec:exp_settings}

We use a residual neural network~\cite{He.etal/2016} to learn a heuristic for a state space. The network's input is a Boolean representation of the states, where a propositional fact is set to~$1$ if it is true in the state and~$0$ otherwise, as explained in Section~\ref{sec:background_learningheuristics}, and its output is a single neuron with the predicted $h$-value. The network has two hidden layers followed by a residual block with two hidden layers. Each hidden layer has~$250$ neurons that use ReLU activation and are initialized as proposed by He et al.~(\citeyear{He.etal/2015}). The training uses the Adam optimizer~\cite{Kingma.Ba/2015}, a learning rate of $10^{-4}$, an early-stop patience of~$100$, and a mean squared error loss function. Due to better results in preliminary experiments, we use batch sizes of $64$ for small and $512$ for large state spaces, and $90\,\%$ of the sampled data as the training set, with the remaining $10\,\%$ as the validation set. In the experiments, different learned heuristics are denoted as $\hat h_A$, where $A$ indicates different algorithmic choices.

We select the domains and tasks from Ferber et al.~(\citeyear{Ferber.etal/2022}), namely: Blocks, Depot, Grid, N-Puzzle, Pipes-NT, Rovers, Scanalyzer, Storage, Transport, and VisitAll. All domains have unit costs except for Scanalyzer and Transport, for which we consider the variant with unit costs. All methods are implemented on the Neural Fast Downward\footnote{Available at \url{https://github.com/PatrickFerber/NeuralFastDownward}} planning system with PyTorch 1.9.0~\cite{Ferber.etal/2020a,Paszke/2019}. All experiments were run on a PC with an AMD Ryzen~9 3900X $12$-core processor, using a single core with $4$~GB RAM per process. We solve all tasks with \gbfs guided by the learned heuristic~\hnn with the lowest generation order as a tie-breaking strategy.

We note that an NN may fail to train if, after initialization, it outputs zero for all training samples (this is called ``born dead'' in Lu et al.~(\citeyear{Lu.etal/2020})). We noticed this occurs with a non-negligible frequency in experiments on smaller state spaces with fewer samples. Thus, we test for this condition, and if, after initialization, the network outputs zero for all samples in the training set, we reinitialize with a different seed until the network outputs a non-zero value for some sample.

We compare the experiments to a baseline \hnnbase with a similar configuration to previous approaches from the literature presented in Section~\ref{sec:background_relatedwork}. For the baseline, the NN is trained by a sampling method that uses random walks with a regression limit of~$200$ backward steps. In addition, mutexes are used during regression and for sample completion, but resetting the $h$-value to $0$ in goal states and the improvement strategies \hmin and \hvfc are turned off.

In preliminary experiments sampling with the FSM algorithm, we use $p_{\bfsrw}\in\{0.01,0.05,0.1,0.2,\ldots,0.9\}$. $p_{\bfsrw}=0.1$ achieved the best average performance\rv{add the expansions of each percentage}. Thus, we fix $p_{\bfsrw}$ to this value in the experiments.

\section{Small State Spaces}
\label{sec:experiment1}

In this section, we study the behavior of different sampling methods on small state spaces. For each domain, we select the task with the largest size state space that can be enumerated completely to obtain \hstar-values. We only select tasks with state spaces with $30$\,K states or more, and fewer than $1$\,M states.  Table~\ref{tab:small-instances} shows the tasks and their state space sizes. For domains Grid, Rovers, Scanalyzer, and Transport, the task had fewer than $30$\,K states, and VisitAll more than $1$\,M, so we manually modified these tasks. We could not find a task within our limits for Depot, Pipes-NT and Storage, so they were excluded from our experiments.
We generate the initial states for the small state spaces by performing a random walk of length~$200$ from the original initial state of a task. Rovers, Scanalyzer and VisitAll had duplicated initial states or that satisfied the goal state. Thus, for these domains, we generate the initial states with random walks of length $25$, $50$, and $8$, respectively.

\input{tables/small-instances}

In all small state space experiments, the coverage for all methods is $100\,\%$. Therefore we use the number of expanded states to evaluate the quality of the heuristic function.  In these experiments, we report means over experiments with five different network seeds and five different sample seeds and $50$ initial states. The training time has been limited to $30$ minutes. If not stated otherwise, methods \bfs, \dfs, \rw and \bfsrw use mutexes, the improvement strategies \hmin and \hvfc, and the number of samples is equal to $1\%$ of the state space size. After initialization, the percentage of networks that output $0$ for all samples was at most $40\,\%$ (in Blocks), and all networks successfully passed the initialization test after the second initialization. Under these conditions, less than $1.5\,\%$ of the NNs did not converge within the time limit.

\subsection{Distribution of States}
\label{sec:experiment1-subset}

Our first set of experiments aims to analyze the influence of the distribution of sampled states in the state space on the quality of the learned heuristics. For this, we compare different sample generation algorithms, regression limits, random sample percentages, and state completion. If not stated otherwise, for the parameters that are not varied we use the baseline setting defined above (regression limit $L=200$,  mutexes, no cost-to-goal improvement methods).

\subsubsection{Sample Generation Algorithms}

In this experiment, we compare four sample generation algorithms: \bfs, \dfs, \rw, and \bfsrw. To control the effect of the cost-to-goal estimates on the quality of the learned heuristic, we replace sample estimates with optimal values $h^{*}$ before training.  Table~\ref{tab:small-h-optimal} shows the number of expanded states of a \gbfs guided by the learned heuristics, and the mean $h^{*}$-values over the sampled states. We see that heuristic \hnnbfs leads to more expanded states than \hnndfs, which in turn expands about $50\,\%$ more states than \hnnrw and \hnnbfsrw, which perform similarly. Using \hnnbfs is significantly worse, and leads in all domains the highest or close to the highest number of expansions. Heuristic \hnndfs has a high number of expansions in Blocks, N-Puzzle and Transport. Looking at the mean $h^{*}$-values, we see that samples generated by \bfs have the lowest, and those by \dfs the highest mean estimates in all domains. Although the distribution of \dfs is closest to that of the whole state space (inferred from the mean $h^{*}$-values), the resulting heuristic expands more states than \rw and \bfsrw, which generate states closer to the goal. Therefore, it seems that multiple random walk rollouts are better than a single rollout with \bfs or \dfs, due to increased sample diversity in different portions of the state space, covering states that are more likely to be visited during the search.

\input{tables/small-h-optimal}

We now compare these results to results shown in Table~\ref{tab:small-h-estimate}, obtained on exactly the same states but using the cost-to-goal estimates obtained during sampling for training the NN. Note that the results for \bfs with estimated costs to the goal differ from those with exact values in Table~\ref{tab:small-h-optimal}. This happens because, during regression with \bfs, the cost-to-goal estimates are only exact on partial states; when turning them to complete states, the estimates can be larger than \hstar. Thus \hnnbfs with the estimates obtained during regression is less informed.

We can see that the relative order of the methods concerning the number of expanded states remains the same, although all methods expand more states. The increase in the number of expanded states is highest for \hnndfs, which expands about seven times more states. In contrast, the other methods expand about twice, meaning that the estimates produced by \dfs during regression are inferior to those produced by the other methods. This is confirmed by the mean $h$-values: we can see that DFS significantly overestimates the true distances. Although \bfs has an estimation quality close to \hstar-value, its expanded states also degrade. These results suggest that sampling more states in localized regions of the state space (\bfs closer to the goal and \dfs more distant from it) is not enough to achieve good results during search with \gbfs.
Furthermore, \hnnbfsrw expands fewer states than \hnnrw and is the best in five of seven domains. Because \hnnbfsrw had a lower increase in expansions compared to \hnnrw, we focus on \bfsrw in the remaining experiments.

\input{tables/small-h-estimate}

\subsubsection{Regression Limit}

In this experiment, we analyze the influence of the regression limit on the number of expanded states of sample generation strategy \bfsrw. We compare a fixed regression limit of $L=200$ to setting the regression limit either to the number of facts $F$, or the number of facts divided by the mean number of effects $\bar F$. Values $F$ and $\bar F$ for the selected tasks, and additionally the largest distance of any state from the goal state~\distfarthest, are shown in Table~\ref{tab:small-strategies-limits}. Both $F$ and $\bar F$ overestimate the largest distance \distfarthest, except for $\bar F$ in three domains (namely Blocks, Scanalyzer and VisitAll). As discussed in Section~\ref{sec:rollout-depth-limit} this is desirable, since random walk rollouts do not follow the shortest paths.

\input{tables/small-strategies-limits}

The right-hand side of Table~\ref{tab:small-strategies-limits} gives the number of expanded states for the three settings of $L$. We see that limits $F$ and $\bar F$ perform better than the fixed limit $200$, with $F$ best on one, $\bar F$ on four, and $200$ on two tasks. Also, when a limit of $200$ is best, the limit $F$ presents the closest results, but when $F$ or $\bar F$ are best, a limit $200$ can be much worse.  Note that $F$ is the best only in the domain where $\bar F$ underestimates \distfarthest.  To validate this, we set $L=\ceil{c\bar F}$ for $c\in\{1.25,1.5,2,2.5,3,3.5\}$ in an additional experiment on domain Blocks. The number of expanded states decreases up to $c=3$ with a mean of $52.31$ expansions. Overall, the adaptive limits $F$ and $\bar F$, although imperfect, are better estimates of the best regression limit.

\subsubsection{Random Samples}

In this experiment, we evaluate the effect of adding randomly generated samples to the sample set, as explained in Section~\ref{sec:random-samples-theory}. We generate sample sets $S=\{(s_1,h_1),\ldots,(s_N,h_N)\}$ where $10\,\%, 20\,\%,\ldots,100\,\%$ are random samples (to which the improvement strategies \hmin and \hvfc are not applied) and the rest is sampled with \bfsrw and a regression limit $\bar F$. Random samples get an $h$-value of $H+1$ where $H=\max_{i\in[N]} h_i$ is the largest $h$-value in samples $S$, except when they are part of the samples, in which case they receive the corresponding estimate (this happens in fewer than $1\,\%$ of the states). Note that when using $100\,\%$ of random samples, each one has the cost-to-goal estimate equal to the regression limit $L+1$ instead of $H+1$, as we do not have samples in $S$.

\input{tables/small-contrasting}

Table~\ref{tab:small-contrasting} shows the performance up to $70\,\%$ random samples. We have omitted the larger values since expansions are higher (namely $56.10$, $73.94$ and $11397.79$). The number of expansions is considerably reduced when using random samples, with $20\,\%$ random samples performing slightly better than other percentages. This also holds for individual domains, except Transport which expands on average a few states more, and N-Puzzle and Rovers, which expand a few states less.

To better understand the effect of random samples, we have performed three additional experiments with $20\,\%$ of random samples. The first focuses on cost-to-goal estimates. We keep the samples but replace $H+1$ by small values, namely either a random $h$-value from the sample set $S$, or a random value drawn from $U[1,5]$. This leads to overall means of $295.65$ and $3832.14$ expanded states, respectively. The second experiment changes the distribution of the random samples: we force them to be part of the \fssp. This leads to a mean of $36.90$ expanded states. Finally, the third experiment does not apply mutexes and leads to a mean of $36.11$ expanded states. From these additional experiments, it is clear that the most relevant factor is a high $h$-value, and the distribution and quality of the states seem to matter less. Overall, the most probable explanation for the effect of random samples seems to be that they help to increase the probability that the search is guided towards samples for which the network has learned good estimates, i.e., the samples obtained through regression.

\subsubsection{State Completion}

Now we focus on how sampled partial states are converted to complete states. In this experiment, all the samples have optimal cost-to-goal estimates \hstar. We compare three different sample completion strategies for a partial state $s$. All of them select a random state from the set of states represented by $s$ or a restriction of it: the set equals either to all states in $s$ (no restrictions), only those states that satisfy mutexes, or only states from the forward state space (perfect baseline).

Table~\ref{tab:small-estimates-mutex} presents the expanded states for these approaches. We can see that applying mutexes has a moderate effect, and is very close to an ideal completion of the states. However, completing randomly also presents competitive results, except for N-Puzzle. Also, N-Puzzle with both restrictions should have similar results -- as for this particular domain all partial states completed respecting the mutexes are part of the \fssp~-- but this is not the case, as N-Puzzle is a particularly noisy domain with a high standard deviation of state expansions, aggravated by \gbfs. To verify this, we ran $900$ experiments ($30$ sample seeds and $30$ network seeds) for N-Puzzle ``Mutex'' and ``\fssp'', and we achieved similar mean expansions of $84.4$ and $88.66$, respectively.

\input{tables/small-estimates-mutex}

\subsection{Estimates of Cost-to-Goal}

Our second set of experiments aims to analyze how techniques used to improve cost-to-goal estimates influence sample quality. To this end, we directly compare the difference of the improved estimates to \hstar-values. Then, to see how well the learned heuristics generalize, we evaluate them over the forward state spaces from Table~\ref{tab:small-instances}.

\subsubsection{Quality of Estimates}

First, we compare the quality of the cost-to-goal estimates to \hstar with and without $h$-value improvement techniques and distinct regression limits, as shown in Table~\ref{tab:small-samples-ssp}. In this table, we show the mean absolute difference between the sample estimates and \hstar, so smaller means indicate better approximations. Note that we are not evaluating the output of an NN, but the estimates part of the sample set.

\input{tables/small-samples-ssp}

The improvement strategies \hmin and \hvfc substantially reduce the estimates for all regression limiting methods. For \default, \facts and \meanfx, using only \hmin reduces the estimates to $31.28$, $13.95$, $4.93$ respectively, and using only \hvfc reduces the estimates to $11.1$, $5.48$, $1.93$ respectively. Thus, \hvfc has the most effect on improving the cost-to-goal estimates compared to \hmin.
Also, the adaptive regression limiting methods are superior to the fixed default \default, and \meanfx has the best results. When comparing \default to \meanfx without $h$-value improvements, the cost-to-goal estimate difference to \hstar decreases by about $6$ times on geometric mean, with Blocks having the best performance, improving more than $25$ times. Finally, using both $h$-value improvements and \meanfx reduces the difference to \hstar from $33.45$ to only $1.60$.

\subsubsection{Evaluation Over the Forward State Spaces}

We now analyze the quality of our learned heuristics and the logic-based heuristics fast-forward \hff~\cite{Hoffmann.Nebel/2001} and goal-count \hgc over all states from the \fssp of each task. Table~\ref{tab:small-strategies-sp-error} shows the results. Except for \hnnbase (baseline), the samples are generated with \bfsrw limited by \default, \facts or \meanfx, and improved with \hmin and \hvfc. The learned heuristic \hnnrs is the same as \hnnbfsrwl{\meanfx}, but $20\,\%$ of the samples are randomly generated. We see that \hnnbfsrwl{\meanfx} reduces the difference of the predicted result to the real one by about $11$ times when compared to \hnnbase, and when compared to \hgc it has the smallest difference in all but two domains. Also, \hnnbfsrwl{\meanfx} presents a similar mean difference to \hff. Note that due to the randomly generated samples in the sample set, \hnnrs doubles the difference compared to \hnnbfsrwl{\meanfx}, with a smaller value only in Blocks.

\input{tables/small-strategies-sp-error}

Comparing Tables~\ref{tab:small-samples-ssp} and \ref{tab:small-strategies-sp-error}, we observe that relative order between \default, \facts and \meanfx is preserved. The mean difference of the samples' estimates to \hstar for \meanfx is $1.60$ in Table~\ref{tab:small-samples-ssp}, and when the corresponding~\hnnbfsrwl{\meanfx} is required to generalize over the entire \fssp, the mean difference is $3.57$.

\subsection{Comparison to Logic-Based Heuristic Functions}

We now compare the NN-based \hnn with logic-based heuristics. The number of expanded states of \gbfs guided by different heuristic functions is shown in Table~\ref{tab:small-samples-heuristic}. The NNs are trained with samples obtained with \bfsrw, all cost-to-goal improvement strategies and regression depth limited by \meanfx.  

First, we see that \hnnbase expands fewer states than \hgc in most domains except Grid, Scanalyzer and VisitAll, but it is far worse than \hff except in Blocks and VisitAll, where the learned heuristic has particularly good results. We also see that \hnnbfsrwl{\meanfx} expands less than \hnnbase in five domains. Note that \hff has better results than \hnnbfsrwl{\meanfx}. However, \hnnbfsrwl{\meanfx} surpasses \hff if $20\,\%$ of the samples are random, or if we increase the budget of \hnnbfsrwl{\meanfx} to $5\,\%$ (instead of $1\,\%$) of the number of states in the \fssp as shown in Table~\ref{tab:small-samples-pct}. This table also indicates that after increasing the budget to $50\,\%$ of the number of states in the \fssp, the gains in quality of the learned heuristic are negligible. Additionally, by comparing Tables~\ref{tab:small-samples-heuristic} and \ref{tab:small-samples-pct}, we see that having fewer samples but \hstar estimates has better results than having more samples but no \hstar estimates, meaning that improving the cost-to-goal estimates is more important than having more samples.

Comparing Tables~\ref{tab:small-strategies-sp-error} and~\ref{tab:small-samples-heuristic}, we see that for the NN-based heuristics, the number of expanded states is linked to the $h$--\hstar difference. An exception is \hnnrs, which has a higher mean difference than \hnnbfsrwl{\meanfx}, but presents the least state expansions, even when compared to \hff. With these results, we conclude that for the samples obtained during regression, having a better generalization over the forward state space is good. In contrast, despite worsening the mean difference to the \fssp, random samples are obtained after the regression procedure and can be helpful due to the reasons discussed in Section~\ref{sec:experiment1-subset}. This means that a smaller $h$-value difference to the \fssp is not a definitive indication of good search quality.

\input{tables/small-samples}

\input{tables/small-samples-pct}

\section{Large State Spaces}
\label{sec:experiment2}

The main goal of the following experiments is to verify our findings from the previous sections on large state spaces, so we compare different configurations of the improved methods with logic-based heuristics and a baseline. We report results over $9$ seeds ($3$ network seeds and $3$ sample seeds), and using $50$ initial states from each of \citeyear{Ferber.etal/2022} moderate tasks, which are the International Planning Competition (IPC) tasks from $10$ selected domains that according to their results are solvable by \gbfs with \hff within $1$ to $900$ seconds. Each domain has the following number of tasks: Blocks, $5$; Depot, $6$; Grid, $2$; N-Puzzle, $8$; Pipes-NT, $10$; Rovers, $8$; Scanalyzer, $6$; Storage, $4$; Transport, $8$; VisitAll, $6$. Details are available in our supplementary material.

The time to generate the samples lasts up to $1$ hour. For training, we set $1$ hour, and each of the $50$ initial states must be solved with \gbfs within $5$ minutes and $2$\,GB RAM, separately. Generally, more samples yield better results; however, because we do not know how much time will be spent on the $h$-value improvement \hvfc phase as it is done after regression, we fix the number of samples at $N = 16\text{M} / |\mathcal{V}|$, which results in a mean of $500$\,MB RAM during sampling and $2$\,GB during \hvfc.

First, we reassess our previous results using the regression limits \facts and \meanfx on large state spaces since our previous experiments (Section \ref{sec:experiment1-subset}) produced similar results. Table~\ref{tab:large-instances-moderate-nomutex} shows the mean coverage and number of expanded states for our methods using \facts or \meanfx, with all $h$-value improvements, along with a model-based and a mutex-free version (denoted by \hnnnomutex). The distinction between model-based and model-free is made because we want to assess the performance of learning over samples that were generated using information from the task or not. Note that despite not using mutexes, the approach with \hnnnomutexl{\meanfx} is still partially model-based as, during sampling, it infers the number of effects from the input task, while \hnnnomutexl{\facts} is a model-free approach.

\input{tables/large-instances-moderate-nomutex}

When comparing the learned heuristic \hnnbfsrwl{\meanfx} over \hnnbfsrwl{\facts}, we see a mean coverage improvement of about $9\,\%$. All domains are improved or have very similar results, except VisitAll, where limiting the regression limit by \facts is better -- this is also observed in the small state space experiments. Without mutexes, the coverage improvements from \hnnnomutexl{\meanfx} over \hnnnomutexl{\facts} are minor, although the smaller number of expanded states indicate samples of higher quality, which achieves expansions close to when using mutexes. With or without mutexes, using \meanfx has the highest positive impact in N-Puzzle, increasing its coverage by about four times. Also, we see that not using mutexes improves the results in Blocks, Depot and Transport, while having a minimal effect in Pipes-NT, Rovers, Scanalyzer and VisitAll, meaning that model-free approaches are promising. From these results, we conclude that \meanfx has better performance than \facts for large state spaces. Therefore, the following experiments will use \meanfx.

Next, we compare the traditional logic-based heuristics \hff and \hgc, the baseline \hnnbase and the best approach \hnnrs to see where NN-based heuristics stand compared to traditional logic-based heuristics. The results are presented in Table~\ref{tab:large-instances-moderate-cmp}.
We see that \hff dominates in most domains, achieving twice the mean coverage of the baseline \hnnbase. However, \hnnrs has only $12\,\%$ less mean coverage than \hff, improving \hnnbase by about $31\,\%$, with competitive coverage in most domains. Note that \hnnrs achieves better mean coverage than \hgc, with higher or equal coverage in $6$ out of $10$ domains. Also, in all domains except Transport, the best-learned heuristic expands fewer states on the same initial states when compared to \hff, indicating that the learned heuristic is more informed and that the inferior coverage is an effect of the slower expansion speed of the NN-based heuristics. Note, however, that the intersection of solved states is biased towards states that are easier to solve. Furthermore, when limiting \hff by the same number of expansions as the learned heuristic, \hff achieves coverage of $81.20$, meaning that it still excels in most states. Because the dataset used contains only tasks that are solvable by \hff within $900$ seconds, the results are also biased towards better performance with a search guided by \hff. 

\input{tables/large-instances-moderate-cmp}

When comparing Tables~\ref{tab:large-instances-moderate-nomutex} and \ref{tab:large-instances-moderate-cmp}, we notice that all NN-based heuristics have similarly poor results in Rovers, independent of configuration. Considering only the learned heuristics, when using $20\,\%$ of random samples (\hnnrs) instead of $0\,\%$ (\hnnbfsrwl{\meanfx}), there are intermediate improvements of about $15\,\%$ in Storage, Transport and VisitAll, and a significant improvement in Pipes-NT, from approximately $18\,\%$ to $80\,\%$ coverage.
