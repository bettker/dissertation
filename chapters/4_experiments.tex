\chapter{Experiments}
\label{sec:experiments}

In this section, we present two sets of experiments. In the first set, we analyze the behavior of sampling methods on planning tasks for which we can enumerate the complete state space with associated perfect cost-to-goal estimates \hstar. We study how different techniques can influence the number of state expansions by a search algorithm. In the second set, we evaluate how our findings generalize to a practical setting with large planning tasks. Our methods are then compared to traditional heuristics and previous works.

\section{Settings}
\label{sec:settings}

We use a residual neural network~\cite{he2016deep} to learn a heuristic for a state space. The network's input is a Boolean representation of the states, where a fact is set to~$1$ if it is true in the state and~$0$ otherwise, as explained in \cref{sec:learning-heuristics}, and its output is a single neuron with the predicted \h-value. The network has two hidden layers followed by a residual block with two hidden layers. Each hidden layer has~$250$ neurons that use ReLU activation and are initialized as proposed by \citet{he2015delving}. The training uses the Adam optimizer~\cite{kingma2015adam}, learning rate of~$10^{-4}$, early-stop patience of~$100$, and MSE loss function. Due to better results in preliminary experiments, we use batch sizes of~$64$ for small and~$512$ for large state spaces. We use $90\,\%$ of the sampled data as the training set, with the remaining $10\,\%$ as the validation set. Different learned heuristics are denoted as~$\hat h_X$, where~$X$ indicates different algorithmic choices.

We select the domains and tasks from \citet{ferber2022neural}: Blocksworld, \mbox{Depot}, Grid, N-Puzzle, Pipesworld-NoTankage, Rovers, Scanalyzer, Storage, Transport, and \mbox{VisitAll}. All domains have unit costs except for Scanalyzer and Transport, for which we consider the variant with unit costs. All methods are implemented on the Neural~Fast~Downward planning system with PyTorch~1.9.0~\cite{ferber2020neural,paszke2019pytorch}. Our source code, planning tasks, and experiments are available\footnote{Available at \url{https://github.com/bettker/NeuralFastDownward}}. All experiments were run on a PC with an AMD~Ryzen~9~3900X processor, using a single core with $4$\,GB RAM per process. The GPU provides a subtle speedup, so it was not used. We solve all tasks with GBFS guided by the heuristic~\h with the lowest generation order as the tie-breaking strategy.

We observe that an NN may fail to train if, after initialization, it outputs zero for all training samples --~referred to as ``born dead'' in \citet{lu2020dying}. This condition arises when the weights and biases of the NN are initialized to consistently map the ReLU activation region to negative values, resulting in a zero gradient and no weight updates during training. This phenomenon occurs with a non-negligible frequency in experiments on smaller state spaces with fewer samples, mainly in the Blocksworld and VisitAll domains. A smaller sample set has limited input variability, reducing the chances of at least one sample correctly activating the ReLU units. In that case, we reinitialize with a different seed until the network outputs a non-zero value for some sample.

To establish a point of comparison for evaluating the performance of the approaches, we propose a baseline \hnnbase. The baseline refers to a neural network configured similarly to previous non-structured NN-based methods described in \cref{sec:related-work-sample-based}. The~\hnnbase is trained using random walks with $L=200$. Mutexes are applied during the regression and for state completion, but resetting the \h-value to zero in goal states and the improvement strategies SAI and SUI are turned off. By comparing the results of our approaches with the baseline, we can assess the potential improvements over existing methods.

To determine a value for $p$, preliminary experiments in small state spaces were performed using $p\in\{0.01,0.05,0.1,0.2,\ldots,0.9\}$. We use the same baseline configuration but with the FSM sampling algorithm. The corresponding geometric mean expansions obtained were $84.92$, $79.51$, $74.05$, $79.46$, $80.39$, $80.79$, $96.17$, $120.42$, $133.99$, $167.14$, and $175.58$, respectively. As a result, we set a fixed value of $p = 0.1$ for the experiments.

\section{Small State Spaces}
\label{sec:small-experiments}

In this section, we study the behavior of different sampling methods on small state spaces. For each domain, we select the task from the IPC benchmarks with the largest state space between $30$\,K and $1$\,M states that can be enumerated completely to obtain \mbox{\hstar-values}. \cref{tab:small-fss-size} shows the tasks and their state space sizes. For domains Grid, Rovers, Scanalyzer, and Transport, the best task found had fewer than $30$\,K states, and VisitAll more than $1$\,M, so we manually modified these tasks. We could not find a non-trivial task within our limits for Depot, Pipesworld-NoTankage, and Storage, so they were excluded from our experiments. We generate the initial states for the small state spaces by performing a random walk of length~$200$ from the original initial state of a task. Rovers, Scanalyzer, and VisitAll, had duplicated initial states or states that satisfied the goal condition. Thus, we generate the initial states for these domains with random walks of length~$25$, $50$, and $8$, respectively.

\begin{table}[tb]
    \caption[Size of the forward state spaces for the selected domains.]{Size of the forward state spaces for the selected small tasks in seven domains. Tasks marked with~$*$ were modified.} 
    \label{tab:small-fss-size}
    \addmargin
    \centering
    \input{tables/small-fss-size}
\end{table}

In the small state space experiments, the coverage for all methods is $100\,\%$. Therefore we use the number of expanded states to evaluate the quality of the heuristic function. In these experiments, we report means over experiments in $25$ models (five different network seeds and five different sample seeds) and $50$~initial states. The training time has been limited to $30$~minutes. After initialization, the percentage of NN that output~$0$ for all samples was at most $40\,\%$~(in Blocksworld), and all networks successfully passed the initialization test after the second initialization. Under these conditions, less than $1.5\,\%$ of the NNs did not converge within the time limit.

We aim to analyze the influence of the distribution of sampled states in the state space on the quality of the learned heuristics and how techniques used to improve cost-to-goal estimates influence sample quality. If not stated otherwise, we use the number of samples equal to $1\%$ of the state space size, regression limit~$L=200$, mutex-based completion, and no cost-to-goal improvement methods.

\subsection{Sample Generation Algorithms}
\label{sec:small-exps-algorithm}

This experiment compares four sample generation algorithms: BFS, DFS, RW, and FSM. To control the cost-to-goal estimates' effect on the learned heuristic's quality, we replace sample estimates with perfect values~\hstar before training. \cref{tab:small-sampling-hstar} shows the number of expanded states of a GBFS guided by the learned heuristics and the mean \hstar-values over the sampled states. We see that heuristic \hnnl{\bfs} leads to more expanded states than \hnnl{\dfs}, which in turn expands about $50\,\%$ more states than \hnnl{\rw} and \hnnl{\bfsrw}, which perform similarly. Using \hnnl{\bfs} is significantly worse and leads to the highest or close to the highest number of expansions in all domains. Heuristic \hnnl{\dfs} has a high number of expansions in Blocksworld, N-Puzzle, and Transport. Looking at the mean \hstar-values, we see that samples generated by BFS have the lowest, and those by DFS the highest mean estimates in all domains. Although the distribution of DFS is closest to that of the whole state space~(inferred from the mean \hstar-values), the resulting heuristic expands more states than RW and FSM, which generate states closer to the goal. Therefore, multiple random walk rollouts seem better than one with BFS or DFS due to increased sample diversity in different portions of the state space, covering states more likely to be visited during the search.

\begin{table}[tb]
    \caption[Comparison of sampling strategies on \hstar-values.]{Comparison of sampling strategies~BFS, DFS, RW, and FSM on \hstar-values. Expanded states of GBFS with learned heuristics and mean \hstar-values over the sample sets.}
    \label{tab:small-sampling-hstar}
    \addmargin
    \centering
    \input{tables/small-sampling-hstar}
\end{table}

We now compare these results to results shown in \cref{tab:small-sampling-h}, obtained on exactly the same states but using the cost-to-goal estimates obtained during sampling for training the NN. Note that the results for BFS with estimated costs to the goal differ from those with exact values in \cref{tab:small-sampling-hstar}. This difference happens because, during regression with BFS, the cost-to-goal estimates are only exact on partial states; when turning them to complete states, the estimates can be larger than \hstar. Thus \hnnl{\bfs} with the estimates obtained during regression is less informed.

We can see that the relative order of the methods concerning the number of expanded states remains the same, although all methods expand more states. The increase in the number of expanded states is highest for \hnnl{\dfs}, which expands about seven times more states. In contrast, the other methods expand about twice as much, meaning that the estimates produced by DFS during regression are inferior to those produced by the other methods. The mean \h-values confirm this: we can see that DFS significantly overestimates the true distances. Although BFS has an estimation quality close to \hstar-value, its expanded states also degrade. These results suggest that sampling more states in localized regions of the state space~(BFS closer to the goal and DFS more distant) is insufficient to achieve good results during the search with GBFS. Furthermore, \hnnl{\bfsrw} expands fewer states than \hnnl{\rw} and is the best in five of seven domains. Because \hnnl{\bfsrw} had a lower increase in expansions compared to \hnnl{\rw}, we use FSM in the remaining experiments.

\begin{table}[tb]
    \caption[Comparison of sampling strategies on estimated \h-values.]{Comparison of sampling strategies BFS, DFS, RW, and FSM on estimated \h-values. Expanded states of GBFS with learned heuristics and mean \h-values over the sample sets.}
    \label{tab:small-sampling-h}
    \addmargin
    \centering
    \input{tables/small-sampling-h}
\end{table}

\subsection{Maximum Regression Limit}
\label{sec:small-exps-rollout-limit}

In this experiment, we analyze the influence of the regression limit on the number of expanded states of the sample generation technique FSM. We compare a fixed regression limit of~$L=200$ with the adaptive rollout limits number of facts~\facts and number of facts divided by the mean number of effects~\meanfx. Values \facts and \meanfx for the selected tasks and the largest distance of any state from a goal state~\ssdiameter are shown in \cref{tab:small-rollout-limit}. Both \facts and \meanfx overestimate the largest distance \ssdiameter, except for \meanfx in three domains~(Blocksworld, Scanalyzer, and VisitAll). As discussed in \cref{sec:rollout-limit}, this is desirable since random walk rollouts do not follow the shortest paths.

\begin{table}[tb]
    \caption[State space information and expanded states with different regression limits.]{State space information and expanded states of GBFS guided by \hnn trained on FSM samples with different regression limits and no cost-to-goal improvements. The value \ssdiameter is the distance of the state most distant from a goal state.}
    \label{tab:small-rollout-limit}
    \addmargin
    \centering
    \input{tables/small-rollout-limit}
\end{table}

The right-hand side of \cref{tab:small-rollout-limit} gives the number of expanded states for the three settings of $L$. We see that limits \facts and \meanfx perform better than the fixed limit~$200$, with \facts best on one, \meanfx on four, and $200$ on two tasks. Also, when a limit of~$200$ is best, \facts presents the closest results, but when \facts or \meanfx are best, a limit~$200$ can be much worse. Note that \facts is the best only in the domain where \meanfx underestimates \ssdiameter. To validate this, we set~$L=\ceil{c\bar F}$ for $c\in\{1.25,1.5,2,2.5,3,3.5\}$ in an additional experiment on domain Blocksworld. The number of expanded states decreases to~$c=3$ with a mean of $52.31$~expansions. Overall, the adaptive limits \facts and \meanfx are better estimates of the best regression limit.

\subsection{Randomly Generated Samples}
\label{sec:small-exps-random-samples}

In this experiment, we evaluate the effect of adding randomly generated samples to the sample set, as explained in \cref{sec:random-samples}. We generate sample sets~$S=\{(s_1,h_1),\ldots,(s_N,h_N)\}$ where $10\,\%, 20\,\%,\ldots,100\,\%$ are random samples and the rest is sampled with FSM and a regression limit~\meanfx. No cost-to-goal improvement is applied. Random samples get a cost-to-goal estimate of~$H+1$ where $H=\max_{i\in[N]} h_i$ is the largest \h-value in samples~$S$, except when they are part of the samples, in which case they receive the corresponding estimate~(this happens in fewer than $1\,\%$ of the samples). Note that when using $100\,\%$ of random samples, each has the cost-to-goal estimate equal to the regression limit~$L+1$ instead of~$H+1$, as we do not have samples in~$S$.

\cref{tab:small-random-samples} shows the performance up to~$70\,\%$ random samples. We have omitted $80\,\%$, $90\,\%$, and $100\,\%$ since expansions are higher~(respectively $56.10$, $73.94$, and $11397.79$). The number of expansions is considerably reduced when using random samples, with $20\,\%$~random samples performing slightly better than other percentages. This phenomenon also holds for individual domains, except Transport which expands on average a few states more, and N-Puzzle and Rovers, which expand a few states less. The results only degrade significantly from $60$\,\%, which makes $50$\,\% a good choice when considering computational resources, as random samples are computationally cheaper to generate than regression samples.

To better understand the effect of random samples, we have performed three additional experiments with $20\,\%$ of random samples. The first focuses on cost-to-goal estimates. We keep the samples but replace $H+1$ with small values: a random \h-value from the sample set~$S$ or a random value drawn from $U[1,5]$. This modification leads to overall means of $295.65$ and $3832.14$ expanded states, respectively. The second experiment changes the distribution of the random samples: we force them to be part of the FSS. This approach leads to a mean of $36.90$ expanded states. Finally, the third experiment does not apply mutexes, leading to a mean of $36.11$ expanded states. From these additional experiments, it is clear that the most relevant factor is a high \h-value, and the distribution and quality of the states seem to matter less. Overall, the most probable explanation for the effect of random samples is that they help to increase the probability that the search is guided towards samples for which the network has learned good estimates, i.e.,~the samples obtained through regression.

\begin{table}[tb]
    \caption[Expanded states from a varying percentage of randomly generated samples.]{Expanded states of GBFS guided by \hnn trained on FSM samples with regression limit~\meanfx, both cost-to-goal improvement strategies, and a varying percentage of randomly generated samples.}
    \label{tab:small-random-samples}
    \addmargin
    \centering
    \input{tables/small-random-samples}
\end{table}

\subsection{State Completion}
\label{sec:small-exps-state-completion}

Here we focus on how sampled partial states are converted to complete states. In this experiment, all the samples have perfect cost-to-goal estimates~\hstar. We compare three different state completion strategies for a partial state~$s$. All of them select a random state from the set of states represented by $s$ or a restriction of it: the set equals either to all states in~$s$~(no restrictions), only those states that satisfy mutexes, or only states from the forward state space~(ideal baseline).

\cref{tab:small-state-completion} presents the expanded states for these approaches. Applying mutexes has a moderate effect and is very close to an ideal completion of the states. However, completing randomly also presents competitive results, except for N-Puzzle. Also, N-Puzzle with both restrictions should have similar results, as for this particular domain, all partial states completed respecting the mutexes are part of the FSS, but this is not the case due to noisy training (detailed in \cref{sec:noisy-training}). To confirm this, we ran $900$ experiments, with $30$~sample seeds and $30$~network seeds, for N-Puzzle ``Mutex'' and ``FSS'', and we achieved similar mean expansions of $84.4$ and $88.66$, respectively.

\begin{table}[tb]
    \caption[Expanded states from different state completion techniques.]{Expanded states of GBFS guided by \hnn trained on FSM samples with \rlmeanfx, \hstar cost-to-goal estimates, and different state completion techniques.}
    \label{tab:small-state-completion}
    \addmargin
    \centering
    \input{tables/small-state-completion}
\end{table}

\subsection{Quality of Estimates}
\label{sec:small-exps-hvalue-quality}

Now, we compare the quality of the cost-to-goal estimates to \hstar with and without cost-to-goal improvement techniques and distinct regression limits, as shown in \cref{tab:small-difference-hstar}. This table shows the mean absolute difference between the sample estimates and \hstar, so smaller means indicate better approximations. Note that we are not evaluating an NN's output but the sample set's cost-to-goal estimates.

The improvement strategies SAI and SUI substantially reduce the estimates for all regression limiting methods. For \rldefault, \rlfacts, and \rlmeanfx, using only SAI reduces the estimates to $31.28$, $13.95$, and $4.93$, respectively, and using only SUI reduces the estimates to $11.1$, $5.48$, and $1.93$ respectively. Thus, SUI has the most effect on improving the cost-to-goal estimates compared to SAI.

The adaptive regression limiting methods are superior to the fixed default~\rldefault, and \rlmeanfx has the best results. When comparing \rldefault to \rlmeanfx without cost-to-goal improvements, the estimate difference to \hstar decreases by about six times on geometric mean. Blocksworld has the best performance, improving more than $25$~times. Finally, using both cost-to-goal improvements and rollout limit \rlmeanfx reduces the difference to \hstar from~$33.45$ to only $1.60$.

\begin{table}[tb]
    \caption[Mean difference of the cost-to-goal estimates to \hstar.]{Mean difference of the cost-to-goal estimates of samples of the sample set to \hstar.}
    \label{tab:small-difference-hstar}
    \addmargin
    \centering
    \input{tables/small-difference-hstar}
\end{table}

\subsection{Evaluation over the Forward State Spaces}
\label{sec:small-exps-hvalue-fss}

We now analyze the quality of our learned heuristics and the traditional heuristics FF~\hff~\cite{hoffmann2001ff} and goal-count~\hgc over all states from the forward state space of each task. \cref{tab:small-heuristics-difference-hstar} shows the results. Except for the baseline \hnnbase, the samples are generated with FSM limited by \rldefault, \rlfacts, or \rlmeanfx and improved with SAI and SUI. The learned heuristic~\hnnrs is the same as \hnnl{\rlmeanfx}, but $20\,\%$ of the samples are randomly generated. We see that \hnnl{\rlmeanfx} reduces the difference of the predicted \h-value to the real one by about $11$ times when compared to \hnnbase. When compared to \hgc, it has the smallest difference in all domains except Scanalyzer. Also, the heuristic \hnnl{\rlmeanfx} presents a similar mean difference to \hff. Due to the randomly generated samples in the sample set, \hnnrs doubles the difference compared to \hnnl{\rlmeanfx}. The only domain with a lower value is Blocksworld, driven by approximately two-thirds of the FSS states having an \hstar-value within a range of two or less the value assigned for random samples, thus improving the average.

\begin{table}[tb]
    \caption[Mean difference of heuristics to \hstar when evaluated over the FSS.]{Mean difference of \hff, \hgc and \hnn, to \hstar when evaluated over the forward state space.}
    \label{tab:small-heuristics-difference-hstar}
    \addmargin
    \centering
    \input{tables/small-heuristics-difference-hstar}
\end{table}

Comparing \cref{tab:small-difference-hstar,tab:small-heuristics-difference-hstar}, we observe that relative order between \rldefault, \rlfacts, and \rlmeanfx is preserved. The mean difference of the samples' estimates to \hstar for \rlmeanfx is $1.60$ in \cref{tab:small-difference-hstar}, and when the corresponding~\hnnl{\rlmeanfx} is required to generalize over the entire FSS, the mean difference is $3.57$.

\subsection{Comparison to Traditional Heuristic Functions}
\label{sec:small-exps-hvalue-comparison}

We now compare the learned heuristics with traditional ones. The number of expanded states of GBFS guided by different heuristic functions is shown in \cref{tab:small-heuristics}. The NNs are trained with samples obtained with FSM (except \hnnrwrs which was sampled by random walk), both cost-to-goal improvement strategies, and regression limit \meanfx.  

\begin{table}[tb]
    \caption[Expanded states of different heuristic functions.]{Expanded states of GBFS with different heuristic functions. The ``\hstar'' column is ideal and only used for comparison.}
    \addmargin
    \label{tab:small-heuristics}
    \centering
    \input{tables/small-heuristics}
\end{table}

\begin{table}[tb]
    \caption[Expanded states from different sample set sizes.]{Expanded states of GBFS with \hnnl{\rlmeanfx} trained with the number of samples corresponding to some percentage of the number of states in the FSS of each task.}
    \label{tab:small-sample-pct}
    \addmargin
    \centering
    \input{tables/small-sample-pct}
\end{table}

First, we see that the baseline \hnnbase expands fewer states than \hgc in most domains except Grid, Scanalyzer, and VisitAll, but it is far worse than \hff except in Blocksworld and VisitAll, where the learned heuristic has particularly good results. We also see that \hnnl{\rlmeanfx} expands less than \hnnbase in five domains. \hff has better results than \hnnl{\rlmeanfx}; however, \hnnl{\rlmeanfx} surpasses \hff if $20\,\%$ of the samples are randomly generated, or if we increase the budget of \hnnl{\rlmeanfx} to $5\,\%$~(instead of $1\,\%$) of the number of states in the FSS as shown in \cref{tab:small-sample-pct}. This table also indicates that after increasing the budget to $50\,\%$ of the number of states in the FSS, the gains in quality of the learned heuristic are negligible. Additionally, by comparing \cref{tab:small-heuristics,tab:small-sample-pct}, we see that having fewer samples but perfect estimates~\hstar has better results than having more samples but \hstar estimates, meaning that improving the cost-to-goal estimates is more important than having more samples.

We see that for the learned heuristics, the order of approaches in terms of \mbox{\h-\hstar}~difference~(\cref{tab:small-heuristics-difference-hstar}) and expanded states~(\cref{tab:small-heuristics}) remains consistent for \hnnbase and \hnnl{\rlmeanfx}, but not for \hnnrs, which has a higher mean difference than \hnnl{\rlmeanfx} but presents the least state expansions, even when compared to \hff. We experimented with the learned heuristic using all our techniques, except for the RW sampling algorithm, to observe the performance gain of our proposed FSM algorithm (\hnnrs) over the RW algorithm (\hnnrwrs). We see that RW's NN outperforms FSM's by ten expansions in Grid but achieves approximately double the expanded states in Blocksworld. Overall, sampling with FSM slightly outperforms RW in other domains, with a geometric mean of $10\,\%$ fewer expansions.

With these results, we conclude that a better generalization over the forward state space is good for the samples obtained during regression. In contrast, despite worsening the mean difference to the FSS, random samples are obtained after the regression procedure and can be helpful due to the reasons discussed in \cref{sec:small-exps-random-samples}. Thus, a smaller \mbox{\h-\hstar}~difference is not a definitive indication of good search quality.

\section{Large State Spaces}
\label{sec:large-experiments}

The main goal of the following experiments is to validate our findings from the previous sections on large state spaces, so we compare different configurations of the improved methods with traditional heuristics and a baseline. We report results over $9$~seeds~($3$~network seeds and $3$~sample seeds), and using $50$~initial states from each of \citet{ferber2022neural} moderate tasks, which are the IPC tasks from ten selected domains that according to their results are solvable by GBFS with \hff within $1$ to $900$~seconds. Each domain has the following number of tasks: Blocksworld,~$5$; Depot,~$6$; Grid,~$2$; N-Puzzle,~$8$; Pipesworld-NoTankage,~$10$; Rovers,~$8$; Scanalyzer,~$6$; Storage,~$4$; Transport,~$8$; VisitAll,~$6$.

We generate samples within one hour and set one hour as the maximum training time. Each of the $50$~initial states must be solved separately with GBFS within $5$~minutes and $2$\,GB~RAM. Generally, more samples yield better results; however, because we do not know how much time will be spent on the cost-to-goal improvement SUI stage as it is done after regression, we fix the number of samples at $N = 16\text{M} / |\mathcal{V}|$, which results in a mean of $500$\,MB RAM during sampling and $2$\,GB during SUI.

First, we reassess our previous results using the regression limits~\rlfacts and \rlmeanfx on large state spaces since our previous experiments~(Section \ref{sec:small-exps-rollout-limit}) produced similar results. \cref{tab:large-rolloutlimit-mutex} shows the mean coverage and number of expanded states for the methods using \rlfacts or \rlmeanfx, with both cost-to-goal improvements. In addition, we explore logic-independent variants~(denoted by \hnnnomutex) that use the state completion technique without mutexes since we want to assess the performance of learning over samples generated with or without information from the task.

\begin{table}[tb]
    \caption[Results of the learned heuristics with and without mutexes.]{Mean coverages and expanded states of the learned heuristics with regression limits and their respective approaches not using mutexes~(\hnnnomutex). Expanded states consider only the initial states solved by all heuristics; Grid, N-Puzzle, and Storage had no common solved initial state. The geometric mean is used for the overall mean of expanded states.}
    \label{tab:large-rolloutlimit-mutex}
    \addmargin
    \centering
    \input{tables/large-rolloutlimit-mutex}
\end{table}

When comparing the learned heuristic~\hnnl{\rlmeanfx} over \hnnl{\rlfacts}, we see a mean coverage improvement of about $9\,\%$. All domains are improved or have very similar results, except VisitAll, where limiting the regression limit by \rlfacts is better -- this is also observed in the small state space experiment. Without mutexes, the coverage improvements from \rlmeanfx over \rlfacts are minor. However, the smaller number of expanded states in \hnnnomutexl{\rlmeanfx} indicates samples of higher quality, which achieves expansions close to when using mutexes. With or without mutexes, using \rlmeanfx has the highest positive effect in N-Puzzle, increasing its coverage by about four times. Also, not using mutexes improves Blocksworld, Depot, and Transport results while having a minimal effect on Pipesworld-NoTankage, Rovers, Scanalyzer, and VisitAll. This result suggests that logic-independent approaches show potential in these domains. Based on the results, we conclude that \rlmeanfx performs better than \rlfacts for large state spaces. Therefore, the following experiments will use \rlmeanfx.

Next, we compare the traditional heuristics~\hff and \hgc, the baseline \hnnbase, and our best approach \hnnrs in \cref{tab:large-heuristics-baseline}. We see that \hff dominates in most domains, achieving twice the mean coverage of the baseline~\hnnbase. However, \hnnrs has only $12\,\%$ less mean coverage than \hff, with competitive coverage in most domains and improving \hnnbase by about $31\,\%$. Note that \hnnrs achieves better mean coverage than \hgc, with higher or equal coverage in $6$ out of $10$~domains. Also, in all domains except Transport, the best-learned heuristic \hnnrs expands fewer states when compared to \hff, indicating that the learned heuristic is more informed and that the inferior coverage is an effect of the slower expansion speed of the learned heuristics. However, the expanded states are biased towards easier tasks, as they refer to commonly solved initial states across the approaches. Furthermore, when limiting \hff by the same number of expansions as the learned heuristic, \hff achieves coverage of $81.20$, meaning that it still excels in most states. Because the dataset used contains only tasks that are solvable by \hff within $900$~seconds, the results are also biased towards better performance with a search guided by \hff. 

\begin{table}[tb]
    \caption[Results of the traditional heuristics, the baseline, and our approach.]{Mean coverages and expanded states of the traditional heuristics \hff and \hgc compared to the baseline learned heuristic \hnnbase and the best learned heuristic \hnnrs, obtained via training over samples with FSM, \rlmeanfx, $20$\,\% of random samples, and both cost-to-goal improvement strategies. Expanded states consider only the initial states solved by all heuristics; N-Puzzle and Storage had no common solved initial state. The geometric mean is used for the overall mean of expanded states.}
    \label{tab:large-heuristics-baseline}
    \addmargin
    \centering
    \input{tables/large-heuristics-baseline}
\end{table}

When comparing \cref{tab:large-rolloutlimit-mutex,tab:large-heuristics-baseline}, we notice that all learned heuristics have similarly poor results in Rovers, independent of configuration. Considering only the learned heuristics, when using $20\,\%$ of random samples~(\hnnrs) instead of $0\,\%$~(\hnnl{\rlmeanfx}), there are intermediate improvements of about $15\,\%$ in Storage, Transport, and VisitAll, and a significant improvement in Pipesworld-NoTankage, from approximately $18\,\%$ to $80\,\%$~coverage.

\section{Comparison to Other Approaches}
\label{sec:large-exps-comparison}

Although we do not aim to systematically compare other methods due to distinct machine configurations or libraries, we try to qualitatively compare our approach with \citet{ferber2022neural} and \citet{otoole2022sampling}. All methods share the same dataset and NN configuration -- differences include batch size, patience value, NN initialization functions, and percentages of data split into training and validation sets.

Regarding our method, the sampling and training procedures can take a combined time of up to $2$~hours, and we use a search time limit of $5$~minutes. \citet{ferber2022neural} perform sampling and training for up to $28$~hours, with a search time limit of $10$~hours. \citet{otoole2022sampling} spend an unreported amount of time to generate $100$\,K~samples and an average of $23$~minutes in training, with a search time-limit of $6$~minutes. \citet{ferber2022neural} use a validation method that consists in retraining the network up to three times if the learned heuristic is not able to solve with GBFS more than $80\,\%$ of generated validation states with a search time-limit of $30$ minutes. \citet{otoole2022sampling} train $10$~networks for each state space and select the one with the best-performing heuristic according to the same validation method as \citet{ferber2022neural}.

Considering only the best configurations, \citet{ferber2022neural} and \citet{otoole2022sampling} perform backward search-based sampling using random walks while we combine breadth-first search with random walk. \citet{ferber2022neural} obtain states through regression and try to solve them using GBFS with the current heuristic to obtain the plans used as training data. They use bootstrapping, which consists of improving the learned heuristics by training with samples of increasing difficulty, starting with a random walk limit of $5$ and doubling it~(up to $8$~times) whenever GBFS finds a plan for at least $95\,\%$ of the states obtained through regression. \citet{otoole2022sampling} perform $5$~rollouts with a regression limit of $L=500$ and use the current depth as cost-to-goal estimates; they use the Tarski planning framework~\cite{tarski2018github} to perform the regression procedure as opposed to Fast~Downward and developed a cost-to-goal improvement techniques equivalent to SAI, and $50\,\%$ of the samples are randomly generated as described in \cref{sec:random-samples}. All methods have a Boolean representation for the samples and use mutexes (obtained from Fast~Downward) to complete undefined variables.

\begin{table}[tb]
    \caption[Mean coverage results of our approach compared to previous works.]{Mean coverage results of \hboot~\cite{ferber2022neural} and \hnrsl~\cite{otoole2022sampling}, with results obtained from their respective papers, and our best learned heuristic trained with $100$\,K~samples, from which $0$\,\%, $20$\,\% and $50$\,\% are randomly generated.}
    \label{tab:large-literature-comparison}
    \addmargin
    \centering
    \input{tables/large-literature-comparison}
\end{table}

We now compare the coverage results between all methods over the same tasks. We perform an additional set of experiments to show in \cref{tab:large-literature-comparison}. We present the results of our best method using $0\,\%$, $20\,\%$ and $50\,\%$ of random samples, obtained from training over $100$\,K~samples -- same quantity as \citet{otoole2022sampling}. We also show results reported by \citet{ferber2022neural}~\hboot and \citet{otoole2022sampling}~\hnrsl, both trained without validation. Note that although the dataset is the same, the results are not fully comparable due to different machine configurations and time dedicated to sampling, training, and testing. Considering coverages, we notice that our methods have results more similar to \hnrsl than to \hboot, and higher coverage, except for Grid, Rovers, Scanalyzer, and Storage. Generating samples only through regression~(i.e.,~without solving states) and training afterward is faster when compared to bootstrapping. A significant limitation of \citet{ferber2020neural} is the high cost of generating samples, as the states generated by the backward random walk must be solved with the currently learned heuristic to produce plans used as samples. (An alternative to computationally cheaper bootstrap is proposed by~\citet{lelis2013cluster}.) Both \hnrsl and our methods suggest that sampling using regression with improvement strategies~(such as SAI, SUI, and random samples) gives competitive results in most domains.

According to \citet{otoole2022sampling}, the proportion of random samples in the sample set has the most positive effect on coverage -- approximately doubling it when going from $0\,\%$ to $50\,\%$ of random samples ($34.7$ vs.~$59.9$, from their supplementary material). As seen in \cref{tab:large-literature-comparison}, we also notice an improvement from using random samples, although smaller. This improvement is more prominent in \citet{otoole2022sampling}, most likely due to their small quantity of rollouts~(only five), negatively influencing sample diversity, which is compensated by adding random samples. Our experiments show that all domains either improve or have similar results, and the mean coverage improves by about $10\,\%$. Also, except for Pipesworld-NoTankage, we saw no improvements above $5\,\%$ using $50\,\%$ of random samples compared to $20\,\%$.

\section{Limitations}
\label{sec:limitations}

This chapter discusses the problems and limitations of methods based on learning heuristics with neural networks. Understanding and addressing these limitations can help advance the field and develop more robust and effective learned heuristic functions.

\subsection{Validation Loss}
\label{sec:validation-loss}

A significant limitation is the unreliability of the validation loss as an indicator of performance during the training. We observe that two NNs trained with identical configuration and regime, varying only in the training seed, produce distinct validation losses, where the network with a larger loss may outperform the one with a smaller loss in terms of coverage or number of states expanded during the search. For instance, when training with \hstar-values over FSM samples, with one sample seed and $100$ network seeds for each of the small state spaces, $36\,\%$ of the models with the least amount of expanded states had a higher validation loss than other models with more expanded states; if the states do not have \hstar-values, the percentage increases to $49\,\%$. Regarding coverage, in the large state space experiments with the heuristic \hnnrs and $9$ runs ($3$ sample seeds and $3$ network seeds) for each state space, $15\,\%$ of the models with the highest coverage had a higher validation loss than the other models with lower coverage. This phenomenon happens because the sampling procedure is imperfect -- an NN learns a limited portion of the state space, which varies per sample seed, so even if the validation loss is small, it provides no guarantees of search quality.

This phenomenon challenges model validation methods and clarifies that a lower validation loss does not necessarily translate to a better heuristic function in the search. Consequently, relying solely on validation loss to measure model quality can lead to misleading conclusions and suboptimal decisions about when to stop training or which NN is most effective for the search. Using multiple seeds can work around the problem. However, alternative evaluation metrics and techniques must be explored to more accurately assess the performance and effectiveness of learned heuristic models before the search.

\subsection{Noisy Training}
\label{sec:noisy-training}

Another limitation to consider is the influence of seed initialization on the training. As noted by other methods that use some form of model validation~\cite{ferber2020neural, shen2020learning, ferber2022neural, otoole2022sampling}, training can be noisy, with multiple runs leading to very different results. A consequence of this is observed in our experiments comparing expanded states, where even a single state that ends up expanded due to an inaccurate heuristic can lead to numerous extra expansions. In contrast, other states can lead the search to a more direct path to the goal. This problem is common for all approaches that use GBFS, but this is further aggravated in NNs that learn from approximated values.

\begin{table}[tb]
    \caption[Expanded states with standard deviations in small state space experiments.]{Expanded states with GBFS and their standard deviations in small state space experiments using the baseline \hnnbase and the best heuristic \hnnrs.}
    \label{tab:small-stdev}
    \addmargin
    \centering
    \input{tables/small-stdev}
\end{table}

\cref{tab:small-stdev} compare the mean number of expansions of the baseline~\hnnbase and~\hnnrs. We see that standard deviation varies per domain, i.e.,~some domains are noisier than others, and training with better-quality samples typically helps. Regarding coverage in large state spaces, the standard deviation was smaller, as it does not vary at the same rate as expanded states. For example, \hnnrs has a Grid coverage of $60$ with a standard deviation of $16$, while the mean standard deviation considering all the other domains is less than $3$.

\subsection{State Representation}
\label{sec:limitation-representation}

This work and others~\cite{ferber2020neural, ferber2022neural, otoole2022sampling} use the same STRIPS representation, and the NN receives as input a vector of Boolean values representing the set of facts of a complete state, where each input neuron corresponds to a fact. However, this can become inefficient when training over large tasks as the input size of the NN grows linearly with the number of facts in the task. Furthermore, sampling methods via regression generate partial states. With the assignment of undefined variables, part of the sampling information is lost.

\citet{yu2020learning} use the same STRIPS representation with a Boolean input for NN but does not complete the undefined variables, assigning false to all $|D(v)|=n$ facts of each undefined variable $v$. Since their undefined variables are represented in \sas, we can infer that at least one of the $n$ facts is true in a complete state. Consequently, by assigning false to all undefined facts, the NN is trained using states that are unreachable during the search process.

On the other hand, \citet{yu2020learning} and \citet{geissmann2015learning} address the \sas representation. They use a multivalued vector to represent a state, which aligns with the internal state representation in Fast~Downward. This choice allows the representation of undefined values and increases the speed of the NN by reducing the dimensionality of the input layer. They discuss that the choice may depend on the domain, and the performance can vary across different domains. However, on average, this approach has shown worse performance than the Boolean representation in experiments on $46$ domains~\cite{yu2020learning}.
